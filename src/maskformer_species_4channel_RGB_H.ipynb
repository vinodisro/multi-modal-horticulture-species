{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e585cc-0c43-4717-bf98-fcb6bb1aad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import MaskFormerForInstanceSegmentation, MaskFormerImageProcessor, MaskFormerConfig\n",
    "from evaluate import load as load_metric\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb  # for logging\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import os\n",
    "import rasterio\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import evaluate\n",
    "# Initialize WandB in offline mode\n",
    "#wandb.init(mode=\"offline\")\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc84444-77ee-4702-8581-8aad88bcdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        for t in self.transforms:\n",
    "            image, mask = t(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "class DualResize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "     \n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        return transforms.functional.resize(img, self.size), transforms.functional.resize(mask, self.size)\n",
    "\n",
    "class DualToPILImage:\n",
    "    def __call__(self, img, mask):\n",
    "        return transforms.ToPILImage()(img), transforms.ToPILImage()(mask)\n",
    "\n",
    "class DualRandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.functional.hflip(img), transforms.functional.hflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class DualRandomVerticalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.functional.vflip(img), transforms.functional.vflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class DualRandomRotation:\n",
    "    def __init__(self, degrees):\n",
    "   \n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        #angle = transforms.RandomRotation.get_params(self.degrees)\n",
    "        #angle = transforms.RandomRotation(10)\n",
    "        angle=10\n",
    "        #print(angle)\n",
    "        return transforms.functional.rotate(img, angle), transforms.functional.rotate(mask, angle)\n",
    "\n",
    "# class ImageColorJitter:\n",
    "#     def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "#         self.transform = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "\n",
    "#     def __call__(self, img, mask):\n",
    "#         return self.transform(img), mask\n",
    "    \n",
    "# class DualNormalize(object):\n",
    "#     def __init__(self, mean, std):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "\n",
    "#     def __call__(self, img, mask):\n",
    "#         img = F.to_tensor(img)\n",
    "#         img = transforms.functional.normalize(img, self.mean, self.std)\n",
    "#         return img, mask\n",
    "\n",
    "class DualNormalize:\n",
    "    def __init__(self, mean, std, height_mean=0, height_std=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean: List of mean values for RGB channels.\n",
    "            std: List of standard deviation values for RGB channels.\n",
    "            height_mean: Mean value for the height channel.\n",
    "            height_std: Standard deviation value for the height channel.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.height_mean = height_mean\n",
    "        self.height_std = height_std\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        # Normalize RGB channels\n",
    "        #print(\"typeeeeeeeeeeeee\",type(img))\n",
    "        #img = F.to_tensor(img)\n",
    "        #img = torch.from_numpy(np.array(img).astype(np.float32))\n",
    "        rgb_channels = img[:3, :, :]\n",
    "        rgb_channels = transforms.functional.normalize(rgb_channels, self.mean, self.std)\n",
    "\n",
    "        # Normalize height channel if it exists\n",
    "        if img.shape[0] == 4:  # Check if the 4th channel exists\n",
    "            height_channel = img[3, :, :]\n",
    "            # Apply dataset-specific standardization\n",
    "            height_channel = (height_channel - self.height_mean) / self.height_std\n",
    "            img = torch.cat([rgb_channels, height_channel.unsqueeze(0)], dim=0)\n",
    "        else:\n",
    "            img = rgb_channels\n",
    "        #print(\"image shape\",img.shape)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9bf182-9c8d-4ef0-b27a-0413a4ce73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944b79ca-47b3-4d5a-97cb-8a925665f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferNormalize:\n",
    "    def __init__(self, mean, std, height_mean=0, height_std=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean: List of mean values for RGB channels.\n",
    "            std: List of standard deviation values for RGB channels.\n",
    "            height_mean: Mean value for the height channel.\n",
    "            height_std: Standard deviation value for the height channel.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.height_mean = height_mean\n",
    "        self.height_std = height_std\n",
    "\n",
    "    def __call__(self, img):\n",
    "       \n",
    "        rgb_channels = img[:3, :, :]\n",
    "        rgb_channels = transforms.functional.normalize(rgb_channels, self.mean, self.std)\n",
    "\n",
    "        # Normalize height channel if it exists\n",
    "        if img.shape[0] == 4:  # Check if the 4th channel exists\n",
    "            height_channel = img[3, :, :]\n",
    "            # Apply dataset-specific standardization\n",
    "            height_channel = (height_channel - self.height_mean) / self.height_std\n",
    "            img = torch.cat([rgb_channels, height_channel.unsqueeze(0)], dim=0)\n",
    "        else:\n",
    "            img = rgb_channels\n",
    "        #print(\"image shape\",img.shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2d74fb-b06f-4eb0-96a3-636be1425c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_mean = [0.485, 0.456, 0.406]\n",
    "# imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]  \n",
    "imagenet_std = [0.229, 0.224, 0.225] \n",
    "height_mean=2.06\n",
    "height_std=3.9\n",
    "\n",
    "\n",
    "#imagenet_mean_4ch = [0.485, 0.456, 0.406]  # Replace with the mean values for your dataset\n",
    "#imagenet_std_4ch = [0.229, 0.224, 0.225]  # Replace with the std values for your dataset\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "train_transform = DualCompose([\n",
    "    DualToPILImage(),\n",
    "    DualResize((512, 512)),  # Example size\n",
    "    DualRandomHorizontalFlip(),\n",
    "    DualRandomVerticalFlip(),\n",
    "    DualRandomRotation(10),  # Example max rotation in degrees\n",
    "    #ImageColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "val_transform = DualCompose([\n",
    "    DualToPILImage(),\n",
    "    DualResize((512, 512)),  # Example size\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "transform = DualCompose([\n",
    "    #DualToPILImage(),\n",
    "    #DualResize((512, 512)),  # Example size\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "455a0675-8003-44b2-882e-5718cdc103af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_to_range(data):\n",
    "#     min_val=data.min()\n",
    "#     max_val=data.max()\n",
    "#     return (data-min_val)/(max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c44530-b787-491f-8b04-3c643efdc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_to_range(data):\n",
    "#     # Assume `data` has shape (C, H, W)\n",
    "#     normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "#     for c in range(data.shape[0]):  # Loop over channels\n",
    "#         min_val = data[c].min()\n",
    "#         max_val = data[c].max()\n",
    "#         if max_val == min_val:\n",
    "#             normalized_data[c] = np.zeros_like(data[c])  # Handle case where all values are the same\n",
    "#         else:\n",
    "#             normalized_data[c] = (data[c] - min_val) / (max_val - min_val)\n",
    "#     return normalized_data *255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2e16eb-569b-4f36-9ee8-68bc18ee8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_range(data):\n",
    "    \"\"\"\n",
    "    Normalize RGB channels to [0, 255] and process height channel separately.\n",
    "    \n",
    "    Args:\n",
    "        data: Numpy array with shape (C, H, W) where C = 4 (R, G, B, Height).\n",
    "        \n",
    "    Returns:\n",
    "        normalized_data: Numpy array with the same shape as `data`.\n",
    "    \"\"\"\n",
    "    normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "    for c in range(data.shape[0]):  # Loop over channels\n",
    "        if c < 3:  # For RGB channels\n",
    "            min_val = data[c].min()\n",
    "            max_val = data[c].max()\n",
    "            if max_val == min_val:\n",
    "                normalized_data[c] = np.zeros_like(data[c])  # Handle case where all values are the same\n",
    "            else:\n",
    "                normalized_data[c] = (data[c] - min_val) / (max_val - min_val) \n",
    "                #normalized_data[c] = data[c]\n",
    "        else:  # For the height channel\n",
    "            height_channel = data[c]\n",
    "            height_channel = np.clip(height_channel, a_min=0, a_max=None)  # Clip negative values to 0\n",
    "            # max_val = height_channel.max()\n",
    "            # if max_val > 0:\n",
    "            #     normalized_data[c] = height_channel / max_val  # Scale to [0, 1]\n",
    "            # else:\n",
    "            normalized_data[c] = height_channel  # All zeros if max_val is 0\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e505698-6ccb-403f-a44d-f31a3267aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = [0,1,2, 3, 4, 5,6,7,8,9,10] \n",
    "class_mapping = {val: idx for idx, val in enumerate(class_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2a9837e-da22-427b-9f09-d83f8f6a9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_mask(mask, mapping):\n",
    "    #print(mask)\n",
    "    # Create a new array to hold the mapped values\n",
    "    remapped_mask = np.zeros_like(mask)\n",
    "    for original, mapped in mapping.items():\n",
    "        remapped_mask[mask == original] = mapped\n",
    "    return remapped_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98c4c40-062b-40fb-9b05-ca2786178aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, mapping,processor, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        #self.image_filenames = os.listdir(image_dir)\n",
    "        self.image_filenames = list(glob.glob(rf\"{image_dir}\\*.tif\"))\n",
    "        #self.mask_filenames = os.listdir(mask_dir)\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print( self.image_filenames[idx])\n",
    "        #image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.basename(image_path))\n",
    "        #mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "        # Use Rasterio to open the Geo-referenced image\n",
    "        with rasterio.open(image_path) as src:\n",
    "            bands=[1, 2, 3, 4]\n",
    "            #bands=[1, 2, 3]\n",
    "            #bands_data = src.read((4,3,2))\n",
    "            bands_data = src.read(bands)\n",
    "            #bands_data[3,:,:]=np.clip(bands_data[3,:,:],a_min=0,a_max=None)\n",
    "            bands_data = bands_data.astype(np.float32) # Convert to float for the normalization\n",
    "\n",
    "            # Normalize each band to [0, 1]\n",
    "            bands_normalized = normalize_to_range(bands_data)\n",
    "\n",
    "            # Rearrange bands into an RGB image\n",
    "            #img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "            img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "            img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)\n",
    "        # Use Rasterio for masks too, assuming they are Geo-referenced as well\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1).astype(np.uint8)  # Read the mask as numpy array\n",
    "            # Remap the mask\n",
    "            mask = remap_mask(mask, self.mapping)\n",
    "           \n",
    "        original_image =img_tensor\n",
    "        #original_segmentation_map =  torch.from_numpy(mask.copy())\n",
    "        original_segmentation_map = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        if self.transform is not None:\n",
    "            img, mask = self.transform(img_tensor, mask)\n",
    "             \n",
    "            #img = F.to_tensor(img)\n",
    "            mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        else:\n",
    "            # Convert numpy arrays to PyTorch tensors\n",
    "            #img = torch.from_numpy(img_tensor)\n",
    "            img= img_tensor\n",
    "            mask = torch.from_numpy(mask.copy())\n",
    "        #print(\"hellllooooooooooooo\",img.shape)\n",
    "\n",
    "        # Process the image and mask using the MaskFormerImageProcessor\n",
    "        #inputs = self.processor(images=img, segmentation_maps=mask, return_tensors=\"pt\")\n",
    "        return (img,mask,original_image, original_segmentation_map)\n",
    "        #return inputs['pixel_values'].squeeze(0), inputs['segmentation_maps'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0abae205-0431-43fc-b71f-6544aa6e3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bg', 1: 'banana', 2: 'coconut', 3: 'dragonfruit', 4: 'guava', 5: 'jackfruit', 6: 'mango', 7: 'sapota', 8: 'papaya', 9: 'othertrees', 10: 'arecanut'}\n"
     ]
    }
   ],
   "source": [
    "#from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "repo_id = f\"segments/sidewalk-semantic\"\n",
    "filename = \"id2label.json\"\n",
    "with open(filename) as class_file:\n",
    "  file_contents = class_file.read()\n",
    "id2label = json.loads(file_contents)\n",
    "#id2label = json.load(open(load_dataset(filename)))\n",
    "id2label = {int(k):v for k,v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f09fa-04f6-413f-8928-16aa8d522f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MaskFormerForInstanceSegmentation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pre-trained MaskFormer model\n",
    "model = MaskFormerForInstanceSegmentation.from_pretrained(\n",
    "    \"facebook/maskformer-swin-base-ade\",\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    num_labels=11,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# Access the Swin Backbone's patch embedding layer\n",
    "patch_embed = model.model.pixel_level_module.encoder.model.embeddings.patch_embeddings\n",
    "original_proj = patch_embed.projection\n",
    "\n",
    "# Original and new input channels\n",
    "original_in_channels = 3\n",
    "new_in_channels = 4  # Example: RGB + Height\n",
    "\n",
    "# Get the number of output channels from the original projection layer\n",
    "out_channels = original_proj.out_channels\n",
    "\n",
    "# Create a new convolution layer with additional input channels\n",
    "new_conv = nn.Conv2d(\n",
    "    new_in_channels,\n",
    "    out_channels,\n",
    "    kernel_size=original_proj.kernel_size,\n",
    "    stride=original_proj.stride,\n",
    "    padding=original_proj.padding,\n",
    "    bias=original_proj.bias is not None,\n",
    ")\n",
    "\n",
    "# Copy the weights from the original convolution layer for the first 3 channels\n",
    "new_conv.weight.data[:, :original_in_channels, :, :] = original_proj.weight.data\n",
    "\n",
    "# Initialize the weights for the new channels (e.g., random or mean of existing weights)\n",
    "new_conv.weight.data[:, original_in_channels:, :, :] = original_proj.weight.data.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Replace the original convolution layer with the new one\n",
    "patch_embed.projection = new_conv\n",
    "\n",
    "print(\"Modified MaskFormer model's patch embedding layer successfully.\")\n",
    "\n",
    "processor = MaskFormerImageProcessor(ignore_index=255,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f658757d-4522-4ed7-a106-418b0b227f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MaskFormerImageProcessor(ignore_index=255,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f926e87-4a7c-4a39-b1e9-c3fece442b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741cedf-c8a7-40d8-ab21-dd4feab2c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor(torch.randn(3, 512, 512), torch.randn(512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5aba29a-d4f8-4399-864a-3a3655e3eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/train/images'\n",
    "train_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/train/masks'\n",
    "val_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/val/images'\n",
    "val_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/val/masks'\n",
    "test_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/test/images'\n",
    "test_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/test/masks'\n",
    "\n",
    "# train_image_dir=r'D:\\pvv\\pvv\\data\\jack\\train\\images'\n",
    "# train_label_dir=r'D:\\pvv\\pvv\\data\\jack\\train\\masks'\n",
    "# val_image_dir=r'D:\\pvv\\pvv\\data\\jack\\val\\images'\n",
    "# val_label_dir=r'D:\\pvv\\pvv\\data\\jack\\val\\masks'\n",
    "#test_image_dir=r'D:\\pvv\\pvv\\data\\test\\images'\n",
    "#test_label_dir=r'D:\\pvv\\pvv\\data\\test\\masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847fee8-da64-4abb-b5f3-2c868e8be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir=r'D:\\pvv\\pvv\\data\\train\\images'\n",
    "train_label_dir=r'D:\\pvv\\pvv\\data\\train\\masks'\n",
    "val_image_dir=r'D:\\pvv\\pvv\\data\\val\\images'\n",
    "val_label_dir=r'D:\\pvv\\pvv\\data\\val\\masks'\n",
    "test_image_dir=r'D:\\pvv\\pvv\\data\\test\\images'\n",
    "test_label_dir=r'D:\\pvv\\pvv\\data\\test\\masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57ae38-e8d3-4057-ac1a-a2974373c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/train/images'\n",
    "train_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/train/masks'\n",
    "val_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/val/images'\n",
    "val_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/val/masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7d2fb93-e920-4a78-b105-bc0cd21755a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    #print(images)\n",
    "    #print(images)\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    #print(images[0].dtype)\n",
    "    \n",
    "    batch = processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "        input_data_format=\"channels_first\"\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    " \n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce6436c1-baf0-49fa-9928-9c394645d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom dataset\n",
    "#image_dir = \"path_to_images\"\n",
    "#mask_dir = \"path_to_masks\"\n",
    "train_dataset = CustomDataset(image_dir=train_image_dir, mask_dir=train_label_dir,mapping=class_mapping,processor=processor, transform=transform)\n",
    "val_dataset = CustomDataset(image_dir=val_image_dir, mask_dir=val_label_dir, mapping=class_mapping,processor=processor, transform=transform)\n",
    "\n",
    "train_subset_size = 300\n",
    "val_subset_size=60\n",
    "train_subset, _ = random_split(train_dataset, [train_subset_size, len(train_dataset) - train_subset_size])\n",
    "val_subset, _ = random_split(val_dataset, [val_subset_size, len(val_dataset) - val_subset_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b609383-111f-4a34-8754-95da6312ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(image_dir=test_image_dir, mask_dir=test_label_dir, mapping=class_mapping,processor=processor, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14305c4e-d0ad-4a3a-83db-4f1a785c170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([4, 4, 512, 512])\n",
      "pixel_mask torch.Size([4, 512, 512])\n",
      "mask_labels torch.Size([1, 512, 512])\n",
      "class_labels torch.Size([1])\n",
      "original_images torch.Size([4, 512, 512])\n",
      "original_segmentation_maps torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75fc3f-2b6b-42eb-ab8a-f96512b35688",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch[\"mask_labels\"]))\n",
    "print(len(batch[\"class_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c26fd-729e-43bd-a8b9-1c1da3d1d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[\"mask_labels\"][0])\n",
    "print(batch[\"class_labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7707-8843-45d8-b534-5ce090b1e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = batch[\"pixel_values\"][0].numpy()\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65194c42-741e-47be-b26b-7b7cb1a76e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d760e-c9d6-4722-a73b-4ac6fd09b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify class labels\n",
    "labels = [id2label[label] for label in batch[\"class_labels\"][0].tolist()]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de41fdf-0ddd-4bd6-9ce3-bfd4ae717329",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(batch[\"mask_labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45294e6-dfd3-48ee-8d33-ce7c2800eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"mask_labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07ffb0-38a2-4db5-a512-12dcfd8a96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(labels, label_name):\n",
    "  print(\"Label:\", label_name)\n",
    "  idx = labels.index(label_name)\n",
    "\n",
    "  visual_mask = (batch[\"mask_labels\"][0][idx].bool().numpy() * 255).astype(np.uint8)\n",
    "  \n",
    "  return Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b767b-b8be-4bb4-b687-d009976a900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mask(labels, \"guava\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b2380-204c-493f-9b71-7d543b747c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch[\"pixel_values\"],\n",
    "                class_labels=batch[\"class_labels\"],\n",
    "                mask_labels=batch[\"mask_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fcf2d-70e9-48c2-9676-6aec3042ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9b743-2639-4dfc-a12a-3169a60a86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "#metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00272131-227c-4c2d-9009-19ce6395a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "#metric = load_metric(\"mean_iou\")\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "#clf_metrics=evaluate.combine([\"accuracy\",\"f1\",\"precision\",\"recall\"])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr= 0.000392,weight_decay=4.3024e-05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "#running_loss = 0.0\n",
    "running_train_loss = 0.0\n",
    "running_val_loss = 0.0\n",
    "num_tr_samples = 0\n",
    "num_val_samples = 0\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "running_train_losses = []\n",
    "running_val_losses = []\n",
    "mean_ious = []\n",
    "mean_accs=[]\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        # Reset the parameter gradients\n",
    "        #pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        #labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "         # forward + backward + optimize\n",
    "        #outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        outputs = model(\n",
    "        pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        # outputs = model(\n",
    "        #   pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        #   mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        #   class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        # )\n",
    "    \n",
    "        # Backward propagation\n",
    "        #loss = outputs.loss\n",
    "        loss.backward()\n",
    "    \n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_train_loss += loss.item()\n",
    "        epoch_train_loss+= loss.item()\n",
    "        num_tr_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Training Loss:\", running_train_loss/num_tr_samples)\n",
    "            running_train_losses.append(running_train_loss/num_tr_samples)\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(tqdm(val_loader)):\n",
    "        # if idx > 5:\n",
    "        #   break\n",
    "    \n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        #labels = batch[\"mask_labels\"]\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "          #outputs = model(pixel_values=pixel_values.to(device))\n",
    "        #   outputs = model(\n",
    "        #   pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        #   mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        #   class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        # )\n",
    "            # outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            # loss, logits = outputs.loss, outputs.logits\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            original_images = batch[\"original_images\"]\n",
    "            target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "            predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "            metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "            #upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            #predicted = upsampled_logits.argmax(dim=1)\n",
    "            #metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=batch[\"mask_labels\"].detach().cpu().numpy())\n",
    "            #clf_metrics.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "        #loss = outputs.loss\n",
    "        #print(loss)\n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_val_loss += loss.item()\n",
    "        epoch_val_loss+= loss.item()\n",
    "        num_val_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"val Loss:\", running_val_loss/num_val_samples)\n",
    "            running_val_losses.append(running_val_loss/num_val_samples)\n",
    "    \n",
    "        \n",
    "        # get original images\n",
    "        #original_images = batch[\"original_images\"]\n",
    "        #target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        #print(\"target size:\",target_sizes)\n",
    "        # predict segmentation maps\n",
    "        #predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)\n",
    "        # get ground truth segmentation maps\n",
    "        #ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "        #print(\"original mask shape\",ground_truth_segmentation_maps[0].numpy().shape)\n",
    "        #print(\"predicted mask shape\",predicted_segmentation_maps[0].numpy().shape)\n",
    "        #metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "    \n",
    "    epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    final_score=metric.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "    #final_cls_score=clf_metrics.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "    mean_ious.append(final_score['mean_iou'])\n",
    "    mean_accs.append(final_score['mean_accuracy'])\n",
    "    #accs.append(final_cls_score['accuaracy'])\n",
    "    print(f\"Epoch {epoch+1} Mean IoU: {final_score['mean_iou']}\")\n",
    "    print(f\"Epoch {epoch+1} Mean Accuracy: {final_score['mean_accuracy']}\")\n",
    "    #print(f\"Epoch {epoch+1} Accuracy: {final_cls_score['accuracy']}\")\n",
    "\n",
    "    # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "    # so if you're interested, feel free to print them as well\n",
    "    #print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d9bb7-1955-4a50-afaa-7694a31de8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(f\"Epoch {epoch+1} classwise iou: {final_score['per_category_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fdb09-a37b-4c4f-b2e3-7ac3652062de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss and IoU curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss curve\n",
    "plt.plot(range(1, num_epochs + 1), epoch_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), epoch_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../../DATA/OUTPUT/MASKFORMER/RGBH/maskformer-swin-base-ade/MASKFORMER_epoch_train_val_losses_species_07-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting running Loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss curve\n",
    "plt.plot(range(1, len(running_train_losses) + 1), running_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, len(running_val_losses) + 1), running_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../../DATA/OUTPUT/MASKFORMER/RGBH/maskformer-swin-base-ade/MASKFORMER_running_train_val_losses_species_07-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Mean IoU curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, num_epochs + 1), mean_ious, label='Mean IoU', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.title('Mean IoU Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../../DATA/OUTPUT/MASKFORMER/RGBH/maskformer-swin-base-ade/IOU_plot_species_07-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597db44c-0570-4635-bfc6-9ff1184a718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model after fine-tuning\n",
    "model.save_pretrained(\"../../../DATA/MODEL/FINAL/maskformer/RGBH/maskformer_swin_base_ade_uav_tree_species_7-12-24.pt\")\n",
    "#processor.save_pretrained(\"path_to_save_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c59045-1ee7-4c7b-960c-e334933f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "for epoch in range(100):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_loader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(val_loader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "  # so if you're interested, feel free to print them as well\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb9a6e-cfea-45bd-b251-4260d556a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('D:/pvv/pvv/data/val/images/iihr_2nd_pass_sub_areca_coco2_12_aug_5.TIF')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f936c4b-45d5-419c-ae1c-71966e9363a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor = SegformerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "\n",
    "# prepare the image for the model\n",
    "#pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "#print(pixel_values.shape)\n",
    "#pixel_values = batch[\"pixel_values\"][0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82ca15-d636-4040-9ea5-4375b835a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf50f2f-a9cc-4567-a250-606c55f74f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = batch[\"pixel_values\"][3][:3, :, :].numpy()\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aafa3-8b99-4983-94db-804fc1699438",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60819-7acc-4d20-baf0-1bb698f12fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(batch[\"pixel_values\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc8b26-7a4d-47df-8b63-35db94958051",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = batch[\"original_images\"]\n",
    "target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "# predict segmentation maps\n",
    "#processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=True, do_rescale=True, do_normalize=True)\n",
    "#processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6060a-524c-416e-8dc3-d4ea3693f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[\"original_images\"][3]\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f94e4-947f-42eb-9e5a-3f06c7abf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_palette():\n",
    "    \"\"\"Color palette that maps each class to RGB values.\n",
    "    \n",
    "    This one is actually taken from ADE20k.\n",
    "    \"\"\"\n",
    "    return [[120, 120, 120], [180, 120, 120], [6, 230, 230], [80, 50, 50],\n",
    "            [4, 200, 3], [120, 120, 80], [140, 140, 140], [204, 5, 255],\n",
    "            [230, 230, 230], [4, 250, 7], [224, 5, 255], [235, 255, 7],\n",
    "            [150, 5, 61], [120, 120, 70], [8, 255, 51], [255, 6, 82],\n",
    "            [143, 255, 140], [204, 255, 4], [255, 51, 7], [204, 70, 3],\n",
    "            [0, 102, 200], [61, 230, 250], [255, 6, 51], [11, 102, 255],\n",
    "            [255, 7, 71], [255, 9, 224], [9, 7, 230], [220, 220, 220],\n",
    "            [255, 9, 92], [112, 9, 255], [8, 255, 214], [7, 255, 224],\n",
    "            [255, 184, 6], [10, 255, 71], [255, 41, 10], [7, 255, 255],\n",
    "            [224, 255, 8], [102, 8, 255], [255, 61, 6], [255, 194, 7],\n",
    "            [255, 122, 8], [0, 255, 20], [255, 8, 41], [255, 5, 153],\n",
    "            [6, 51, 255], [235, 12, 255], [160, 150, 20], [0, 163, 255],\n",
    "            [140, 140, 140], [250, 10, 15], [20, 255, 0], [31, 255, 0],\n",
    "            [255, 31, 0], [255, 224, 0], [153, 255, 0], [0, 0, 255],\n",
    "            [255, 71, 0], [0, 235, 255], [0, 173, 255], [31, 0, 255],\n",
    "            [11, 200, 200], [255, 82, 0], [0, 255, 245], [0, 61, 255],\n",
    "            [0, 255, 112], [0, 255, 133], [255, 0, 0], [255, 163, 0],\n",
    "            [255, 102, 0], [194, 255, 0], [0, 143, 255], [51, 255, 0],\n",
    "            [0, 82, 255], [0, 255, 41], [0, 255, 173], [10, 0, 255],\n",
    "            [173, 255, 0], [0, 255, 153], [255, 92, 0], [255, 0, 255],\n",
    "            [255, 0, 245], [255, 0, 102], [255, 173, 0], [255, 0, 20],\n",
    "            [255, 184, 184], [0, 31, 255], [0, 255, 61], [0, 71, 255],\n",
    "            [255, 0, 204], [0, 255, 194], [0, 255, 82], [0, 10, 255],\n",
    "            [0, 112, 255], [51, 0, 255], [0, 194, 255], [0, 122, 255],\n",
    "            [0, 255, 163], [255, 153, 0], [0, 255, 10], [255, 112, 0],\n",
    "            [143, 255, 0], [82, 0, 255], [163, 255, 0], [255, 235, 0],\n",
    "            [8, 184, 170], [133, 0, 255], [0, 255, 92], [184, 0, 255],\n",
    "            [255, 0, 31], [0, 184, 255], [0, 214, 255], [255, 0, 112],\n",
    "            [92, 255, 0], [0, 224, 255], [112, 224, 255], [70, 184, 160],\n",
    "            [163, 0, 255], [153, 0, 255], [71, 255, 0], [255, 0, 163],\n",
    "            [255, 204, 0], [255, 0, 143], [0, 255, 235], [133, 255, 0],\n",
    "            [255, 0, 235], [245, 0, 255], [255, 0, 122], [255, 245, 0],\n",
    "            [10, 190, 212], [214, 255, 0], [0, 204, 255], [20, 0, 255],\n",
    "            [255, 255, 0], [0, 153, 255], [0, 41, 255], [0, 255, 204],\n",
    "            [41, 0, 255], [41, 255, 0], [173, 0, 255], [0, 245, 255],\n",
    "            [71, 0, 255], [122, 0, 255], [0, 255, 184], [0, 92, 255],\n",
    "            [184, 255, 0], [0, 133, 255], [255, 214, 0], [25, 194, 194],\n",
    "            [102, 255, 0], [92, 0, 255]]\n",
    "\n",
    "palette = color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f53f85-b7f7-4d80-aaf6-15cce6dc4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n",
    "print(segmentation_map)\n",
    "\n",
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_segmentation_map[segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "ground_truth_color_seg = color_segmentation_map[..., ::-1]\n",
    "#image = np.transpose(image, [2, 1, 0])\n",
    "#img = image * 0.5 + ground_truth_color_seg * 0.5\n",
    "#img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(ground_truth_color_seg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c5d57-bb91-471e-9427-31b6349aefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segmentation_map = batch[\"original_segmentation_maps\"][2]\n",
    "\n",
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_segmentation_map[segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "ground_truth_color_seg = color_segmentation_map[..., ::-1]\n",
    "\n",
    "#img = image * 0.5 + ground_truth_color_seg * 0.5\n",
    "#img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(segmentation_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73b511-d48d-4400-b60b-00761f64a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f25563-b4c9-42b7-8b9f-0d6688ad8769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backbone maskformer-swin is not a supported model and may not be compatible with MaskFormer. Supported model types: resnet,swin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the modified 4-channel MaskFormer model.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def modify_patch_embedding(model, new_in_channels=4):\n",
    "    # Access the patch embedding layer\n",
    "    patch_embed = model.model.pixel_level_module.encoder.model.embeddings.patch_embeddings\n",
    "    original_proj = patch_embed.projection\n",
    "\n",
    "    # Create a new convolution layer with updated input channels\n",
    "    new_conv = torch.nn.Conv2d(\n",
    "        new_in_channels,\n",
    "        original_proj.out_channels,\n",
    "        kernel_size=original_proj.kernel_size,\n",
    "        stride=original_proj.stride,\n",
    "        padding=original_proj.padding,\n",
    "        bias=original_proj.bias is not None,\n",
    "    )\n",
    "\n",
    "    # Initialize new weights\n",
    "    new_conv.weight.data[:, :original_proj.in_channels, :, :] = original_proj.weight.data\n",
    "    new_conv.weight.data[:, original_proj.in_channels:, :, :] = original_proj.weight.data.mean(dim=1, keepdim=True)\n",
    "\n",
    "    # Replace the original projection layer\n",
    "    patch_embed.projection = new_conv\n",
    "\n",
    "# Load the configuration\n",
    "model_dir=\"../../../DATA/MODEL/FINAL/MASKFORMER/RGBH/maskformer_swin_large_ade_uav_tree_species_7-12-24.pt\"\n",
    "#model_dir = \"maskformer_swin_base_ade_uav_tree_species.pt\"\n",
    "config = MaskFormerConfig.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "# Instantiate the model (with a default architecture)\n",
    "model = MaskFormerForInstanceSegmentation(config)\n",
    "\n",
    "# Modify the architecture for 4-channel input\n",
    "modify_patch_embedding(model)\n",
    "\n",
    "# Load the safetensors state dictionary\n",
    "from safetensors import safe_open\n",
    "\n",
    "safetensors_path = f\"{model_dir}/model.safetensors\"\n",
    "with safe_open(safetensors_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    state_dict = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# Load the modified state dictionary into the model\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model=model.to(device)\n",
    "print(\"Successfully loaded the modified 4-channel MaskFormer model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34419e72-00b3-4372-a9dd-8d0b906cf21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = MaskFormerImageProcessor(ignore_index=255,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab63d67-6f10-4fe4-aaab-5737781b7de5",
   "metadata": {},
   "source": [
    "## Estimate model paprameters and memory for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed09da80-34b4-4944-aa95-1e3e415dfe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 211546048\n",
      "Memory for Weights: 806.98 MB\n"
     ]
    }
   ],
   "source": [
    "#from transformers import SegformerForSemanticSegmentation\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "memory_weights = total_params * 4 / (1024 ** 2)  # In MB for float32\n",
    "print(f\"Memory for Weights: {memory_weights:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fcf07-f3fc-4599-9c48-2438f8c25e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, we'll use augmentations\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "#model_path = \"../DATA/MODEL/species_model_unet_16-10-24.pt\"\n",
    "def predict(model, image_path, device):\n",
    "    \n",
    "    # transform = transforms.Compose([\n",
    "    # transforms.Resize((512, 512)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    # For training, we'll use augmentations\n",
    "    transform = InferCompose([\n",
    "        InferNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "        #transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    #image = Image.open(test_image_path).convert(\"RGB\")\n",
    "    \n",
    "     # Use Rasterio to open the Geo-referenced image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        \n",
    "        bands=[1,2,3,4]\n",
    "        #bands=[1,2,3]\n",
    "        #bands = src.read((4,3,2))\n",
    "        bands = src.read(bands)\n",
    "        bands = bands.astype(np.float32) # Convert to float for the normalization\n",
    "\n",
    "        # Normalize each band to [0, 1]\n",
    "        bands_normalized = normalize_to_range(bands)\n",
    "\n",
    "        # Rearrange bands into an RGB image\n",
    "        img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "        img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)    \n",
    "        img_tensor = transform(img_tensor).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
    "    \n",
    "    # Pass the preprocessed image through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seg_output= model(img_tensor)\n",
    "       \n",
    "    \n",
    "       \n",
    "    return seg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9150740-5b03-4b0c-8303-922257789f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/test/images/IIHR_TR_SAMPLES_CLIP_2ND_PASS_ARECA_PAPAYA_615_aug_6.TIF\"\n",
    "test_mask_path = \"E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/test/masks/IIHR_TR_SAMPLES_CLIP_2ND_PASS_ARECA_PAPAYA_615_aug_6.TIF\"\n",
    "predicted_mask = predict(model, test_image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97249e-eee0-43f7-8b5c-7a90ffbc68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segmentation_maps = processor.post_process_semantic_segmentation(predicted_mask, target_sizes=[(512,512)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eda1e01-0c40-4efe-8ecf-3ec6ecccc67c",
   "metadata": {},
   "source": [
    "## Model Testing with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6cd512-1854-4c56-9d7b-3398eebef4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "f1_metric=evaluate.load(\"f1\")\n",
    "precision_metric=evaluate.load(\"precision\")\n",
    "recall_metric=evaluate.load(\"recall\")\n",
    "\n",
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_loader)):\n",
    "    # if idx > 5:\n",
    "    #   break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    #labels = batch[\"mask_labels\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        #loss = outputs.loss\n",
    "        original_images = batch[\"original_images\"]\n",
    "        target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                              target_sizes=target_sizes)\n",
    "        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        ground_truth_segmentation_maps = torch.cat(\n",
    "            [gt_map.flatten() for gt_map in ground_truth_segmentation_maps], dim=0\n",
    "        ).cpu().numpy().astype('int32')\n",
    "        \n",
    "        predicted_segmentation_maps = torch.cat(\n",
    "            [torch.tensor(pred_map).flatten() for pred_map in predicted_segmentation_maps], dim=0\n",
    "        ).cpu().numpy().astype('int32')\n",
    "        #print(type(ground_truth_segmentation_maps))\n",
    "        #print(predicted_segmentation_maps)\n",
    "        f1_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        precision_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        recall_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "\n",
    "final_score=metric.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "final_f1_score=f1_metric.compute(average=None)\n",
    "#mean_f1_score=f1_metric.compute(average=\"macro\")\n",
    "final_precision_score=precision_metric.compute(average=None)\n",
    "#mean_precision_score=precision_metric.compute(average=\"macro\")\n",
    "final_recall_score=recall_metric.compute(average=None)\n",
    "#mean_recall_score=precision_metric.compute(average=\"macro\")\n",
    "#final_cls_score=clf_metrics.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "#mean_ious.append(final_score['mean_iou'])\n",
    "#mean_accs.append(final_score['mean_accuracy'])\n",
    "#accs.append(final_cls_score['accuaracy'])\n",
    "print(f\"Mean IoU: {final_score['mean_iou']}\")\n",
    "print(f\"Mean Accuracy: {final_score['mean_accuracy']}\")\n",
    "print(f\"Classwise iou: {final_score['per_category_iou']}\")\n",
    "print(f\"Classwise accuracy: {final_score['per_category_accuracy']}\")\n",
    "print(f\"Classwise F1: {final_f1_score['f1']}\")\n",
    "print(f\"Mean F1 score: {final_f1_score['f1'].mean()}\")\n",
    "print(f\"Classwise Precision: {final_precision_score['precision']}\")\n",
    "print(f\"Mean Precision score: {final_precision_score['precision'].mean()}\")\n",
    "print(f\"Classwise Recall: {final_recall_score['recall']}\")\n",
    "print(f\"Mean Recall score: {final_recall_score['recall'].mean()}\")\n",
    "#final_score['per_category_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bac50-da8f-4cfd-9559-a33524b379d8",
   "metadata": {},
   "source": [
    "## Inference time performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0fbe4f6-4c7b-4bf9-9269-43e1b4d971f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563c8248c87f4468a72f3bb65b88d766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inference Time: 68.95 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "total_time = 0\n",
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_loader)):\n",
    "    # if idx > 5:\n",
    "    #   break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    #labels = batch[\"mask_labels\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40818bd-1e17-4d3a-a995-7a16749185c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency per Batch: 0.28 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Latency per Batch: {total_time / len(test_loader):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98be053-a45e-46fa-8398-9c64045e670c",
   "metadata": {},
   "source": [
    "## Per-Sample Latency: Compute the average time per sample:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef0de33c-e911-454b-afe6-189481e32e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Latency per Sample: 70.21 ms\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(test_dataset)\n",
    "avg_latency = total_time / num_samples\n",
    "print(f\"Average Latency per Sample: {avg_latency * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedc0826-3276-45f1-9ba7-ff7f2d98cace",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
