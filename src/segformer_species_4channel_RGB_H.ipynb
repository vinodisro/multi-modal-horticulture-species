{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0e585cc-0c43-4717-bf98-fcb6bb1aad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SegformerImageProcessor\n",
    "from transformers import SegformerForSemanticSegmentation,SegformerConfig\n",
    "#from evaluate import load as load_metric\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb  # for logging\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import os\n",
    "import rasterio\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "#import evaluate\n",
    "# Initialize WandB in offline mode\n",
    "#wandb.init(mode=\"offline\")\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc84444-77ee-4702-8581-8aad88bcdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        for t in self.transforms:\n",
    "            image, mask = t(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "class DualResize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "     \n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        return transforms.functional.resize(img, self.size), transforms.functional.resize(mask, self.size)\n",
    "\n",
    "class DualToPILImage:\n",
    "    def __call__(self, img, mask):\n",
    "        return transforms.ToPILImage()(img), transforms.ToPILImage()(mask)\n",
    "\n",
    "class DualRandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.functional.hflip(img), transforms.functional.hflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class DualRandomVerticalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.functional.vflip(img), transforms.functional.vflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class DualRandomRotation:\n",
    "    def __init__(self, degrees):\n",
    "   \n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        #angle = transforms.RandomRotation.get_params(self.degrees)\n",
    "        #angle = transforms.RandomRotation(10)\n",
    "        angle=10\n",
    "        #print(angle)\n",
    "        return transforms.functional.rotate(img, angle), transforms.functional.rotate(mask, angle)\n",
    "\n",
    "# class ImageColorJitter:\n",
    "#     def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "#         self.transform = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "\n",
    "#     def __call__(self, img, mask):\n",
    "#         return self.transform(img), mask\n",
    "    \n",
    "# class DualNormalize(object):\n",
    "#     def __init__(self, mean, std):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "\n",
    "#     def __call__(self, img, mask):\n",
    "#         img = F.to_tensor(img)\n",
    "#         img = transforms.functional.normalize(img, self.mean, self.std)\n",
    "#         return img, mask\n",
    "\n",
    "class DualNormalize:\n",
    "    def __init__(self, mean, std, height_mean=0, height_std=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean: List of mean values for RGB channels.\n",
    "            std: List of standard deviation values for RGB channels.\n",
    "            height_mean: Mean value for the height channel.\n",
    "            height_std: Standard deviation value for the height channel.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.height_mean = height_mean\n",
    "        self.height_std = height_std\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        # Normalize RGB channels\n",
    "        #print(\"typeeeeeeeeeeeee\",type(img))\n",
    "        #img = F.to_tensor(img)\n",
    "        #img = torch.from_numpy(np.array(img).astype(np.float32))\n",
    "        rgb_channels = img[:3, :, :]\n",
    "        rgb_channels = transforms.functional.normalize(rgb_channels, self.mean, self.std)\n",
    "\n",
    "        # Normalize height channel if it exists\n",
    "        if img.shape[0] == 4:  # Check if the 4th channel exists\n",
    "            height_channel = img[3, :, :]\n",
    "            # Apply dataset-specific standardization\n",
    "            height_channel = (height_channel - self.height_mean) / self.height_std\n",
    "            img = torch.cat([rgb_channels, height_channel.unsqueeze(0)], dim=0)\n",
    "        else:\n",
    "            img = rgb_channels\n",
    "        #print(\"image shape\",img.shape)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9bf182-9c8d-4ef0-b27a-0413a4ce73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944b79ca-47b3-4d5a-97cb-8a925665f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferNormalize:\n",
    "    def __init__(self, mean, std, height_mean=0, height_std=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean: List of mean values for RGB channels.\n",
    "            std: List of standard deviation values for RGB channels.\n",
    "            height_mean: Mean value for the height channel.\n",
    "            height_std: Standard deviation value for the height channel.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.height_mean = height_mean\n",
    "        self.height_std = height_std\n",
    "\n",
    "    def __call__(self, img):\n",
    "       \n",
    "        rgb_channels = img[:3, :, :]\n",
    "        rgb_channels = transforms.functional.normalize(rgb_channels, self.mean, self.std)\n",
    "\n",
    "        # Normalize height channel if it exists\n",
    "        if img.shape[0] == 4:  # Check if the 4th channel exists\n",
    "            height_channel = img[3, :, :]\n",
    "            # Apply dataset-specific standardization\n",
    "            height_channel = (height_channel - self.height_mean) / self.height_std\n",
    "            img = torch.cat([rgb_channels, height_channel.unsqueeze(0)], dim=0)\n",
    "        else:\n",
    "            img = rgb_channels\n",
    "        #print(\"image shape\",img.shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2d74fb-b06f-4eb0-96a3-636be1425c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_mean = [0.485, 0.456, 0.406]\n",
    "# imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]  \n",
    "imagenet_std = [0.229, 0.224, 0.225] \n",
    "height_mean=2.06\n",
    "height_std=3.9\n",
    "\n",
    "\n",
    "#imagenet_mean_4ch = [0.485, 0.456, 0.406]  # Replace with the mean values for your dataset\n",
    "#imagenet_std_4ch = [0.229, 0.224, 0.225]  # Replace with the std values for your dataset\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "train_transform = DualCompose([\n",
    "    DualToPILImage(),\n",
    "    DualResize((512, 512)),  # Example size\n",
    "    DualRandomHorizontalFlip(),\n",
    "    DualRandomVerticalFlip(),\n",
    "    DualRandomRotation(10),  # Example max rotation in degrees\n",
    "    #ImageColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "val_transform = DualCompose([\n",
    "    DualToPILImage(),\n",
    "    DualResize((512, 512)),  # Example size\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "transform = DualCompose([\n",
    "    #DualToPILImage(),\n",
    "    #DualResize((512, 512)),  # Example size\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "455a0675-8003-44b2-882e-5718cdc103af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_to_range(data):\n",
    "#     min_val=data.min()\n",
    "#     max_val=data.max()\n",
    "#     return (data-min_val)/(max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c44530-b787-491f-8b04-3c643efdc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_to_range(data):\n",
    "#     # Assume `data` has shape (C, H, W)\n",
    "#     normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "#     for c in range(data.shape[0]):  # Loop over channels\n",
    "#         min_val = data[c].min()\n",
    "#         max_val = data[c].max()\n",
    "#         if max_val == min_val:\n",
    "#             normalized_data[c] = np.zeros_like(data[c])  # Handle case where all values are the same\n",
    "#         else:\n",
    "#             normalized_data[c] = (data[c] - min_val) / (max_val - min_val)\n",
    "#     return normalized_data *255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e2e16eb-569b-4f36-9ee8-68bc18ee8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_range(data):\n",
    "    \"\"\"\n",
    "    Normalize RGB channels to [0, 255] and process height channel separately.\n",
    "    \n",
    "    Args:\n",
    "        data: Numpy array with shape (C, H, W) where C = 4 (R, G, B, Height).\n",
    "        \n",
    "    Returns:\n",
    "        normalized_data: Numpy array with the same shape as `data`.\n",
    "    \"\"\"\n",
    "    normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "    for c in range(data.shape[0]):  # Loop over channels\n",
    "        if c < 3:  # For RGB channels\n",
    "            min_val = data[c].min()\n",
    "            max_val = data[c].max()\n",
    "            if max_val == min_val:\n",
    "                normalized_data[c] = np.zeros_like(data[c])  # Handle case where all values are the same\n",
    "            else:\n",
    "                normalized_data[c] = (data[c] - min_val) / (max_val - min_val) \n",
    "                #normalized_data[c] = data[c]\n",
    "        else:  # For the height channel\n",
    "            height_channel = data[c]\n",
    "            height_channel = np.clip(height_channel, a_min=0, a_max=None)  # Clip negative values to 0\n",
    "            # max_val = height_channel.max()\n",
    "            # if max_val > 0:\n",
    "            #     normalized_data[c] = height_channel / max_val  # Scale to [0, 1]\n",
    "            # else:\n",
    "            normalized_data[c] = height_channel  # All zeros if max_val is 0\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e505698-6ccb-403f-a44d-f31a3267aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = [0,1,2, 3, 4, 5,6,7,8,9,10] \n",
    "class_mapping = {val: idx for idx, val in enumerate(class_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2a9837e-da22-427b-9f09-d83f8f6a9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_mask(mask, mapping):\n",
    "    #print(mask)\n",
    "    # Create a new array to hold the mapped values\n",
    "    remapped_mask = np.zeros_like(mask)\n",
    "    for original, mapped in mapping.items():\n",
    "        remapped_mask[mask == original] = mapped\n",
    "    return remapped_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98c4c40-062b-40fb-9b05-ca2786178aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, mapping,processor, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        #self.image_filenames = os.listdir(image_dir)\n",
    "        self.image_filenames = list(glob.glob(rf\"{image_dir}\\*.tif\"))\n",
    "        #self.mask_filenames = os.listdir(mask_dir)\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print( self.image_filenames[idx])\n",
    "        #image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.basename(image_path))\n",
    "        #mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "        # Use Rasterio to open the Geo-referenced image\n",
    "        with rasterio.open(image_path) as src:\n",
    "            bands=[1, 2, 3, 4]\n",
    "            #bands=[1, 2, 3]\n",
    "            #bands_data = src.read((4,3,2))\n",
    "            bands_data = src.read(bands)\n",
    "            #bands_data[3,:,:]=np.clip(bands_data[3,:,:],a_min=0,a_max=None)\n",
    "            bands_data = bands_data.astype(np.float32) # Convert to float for the normalization\n",
    "\n",
    "            # Normalize each band to [0, 1]\n",
    "            bands_normalized = normalize_to_range(bands_data)\n",
    "\n",
    "            # Rearrange bands into an RGB image\n",
    "            #img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "            img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "            img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)\n",
    "        # Use Rasterio for masks too, assuming they are Geo-referenced as well\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1).astype(np.uint8)  # Read the mask as numpy array\n",
    "            # Remap the mask\n",
    "            mask = remap_mask(mask, self.mapping)\n",
    "           \n",
    "        original_image =img_tensor\n",
    "        #original_segmentation_map =  torch.from_numpy(mask.copy())\n",
    "        original_segmentation_map = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        if self.transform is not None:\n",
    "            img, mask = self.transform(img_tensor, mask)\n",
    "             \n",
    "            #img = F.to_tensor(img)\n",
    "            mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        else:\n",
    "            # Convert numpy arrays to PyTorch tensors\n",
    "            #img = torch.from_numpy(img_tensor)\n",
    "            img= img_tensor\n",
    "            mask = torch.from_numpy(mask.copy())\n",
    "        #print(\"hellllooooooooooooo\",img.shape)\n",
    "\n",
    "        # Process the image and mask using the MaskFormerImageProcessor\n",
    "        #inputs = self.processor(images=img, segmentation_maps=mask, return_tensors=\"pt\")\n",
    "        return (img,mask,original_image, original_segmentation_map)\n",
    "        #return inputs['pixel_values'].squeeze(0), inputs['segmentation_maps'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0abae205-0431-43fc-b71f-6544aa6e3eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bg', 1: 'banana', 2: 'coconut', 3: 'dragonfruit', 4: 'guava', 5: 'jackfruit', 6: 'mango', 7: 'sapota', 8: 'papaya', 9: 'othertrees', 10: 'arecanut'}\n"
     ]
    }
   ],
   "source": [
    "#from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "repo_id = f\"segments/sidewalk-semantic\"\n",
    "filename = \"id2label.json\"\n",
    "with open(filename) as class_file:\n",
    "  file_contents = class_file.read()\n",
    "id2label = json.loads(file_contents)\n",
    "#id2label = json.load(open(load_dataset(filename)))\n",
    "id2label = {int(k):v for k,v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f09fa-04f6-413f-8928-16aa8d522f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MaskFormerForInstanceSegmentation\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained SegFormer\n",
    "model_name = \"nvidia/mit-b4\"\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(model_name,num_labels=11,id2label=id2label,label2id=label2id,local_files_only=True)\n",
    "\n",
    "# Access the patch embeddings layer\n",
    "original_in_channels = 3\n",
    "new_in_channels = 4  # Example: RGB + Height + Roughness + Gradient\n",
    "original_patch_embedding = model.segformer.encoder.patch_embeddings[0]\n",
    "\n",
    "# Access the internal convolution layer\n",
    "original_proj = original_patch_embedding.proj\n",
    "\n",
    "# Get the number of output channels from the model configuration\n",
    "out_channels = model.config.hidden_sizes[0]\n",
    "\n",
    "# Create a new convolution layer with additional input channels\n",
    "new_conv = nn.Conv2d(\n",
    "    new_in_channels,\n",
    "    out_channels,\n",
    "    kernel_size=original_proj.kernel_size,\n",
    "    stride=original_proj.stride,\n",
    "    padding=original_proj.padding,\n",
    "    bias=original_proj.bias is not None,\n",
    ")\n",
    "\n",
    "# Copy the weights from the original convolution layer for the first 3 channels\n",
    "new_conv.weight.data[:, :original_in_channels, :, :] = original_proj.weight.data\n",
    "# Initialize the weights for the new channels (e.g., random or mean of existing weights)\n",
    "new_conv.weight.data[:, original_in_channels:, :, :] = original_proj.weight.data.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Replace the original convolution layer with the new one\n",
    "original_patch_embedding.proj = new_conv\n",
    "\n",
    "print(\"Modified model's patch embedding layer successfully.\")\n",
    "processor = SegformerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0808a750-7d05-4aff-8dc0-55ef05041031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'ignore_index'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "processor = SegformerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aba29a-d4f8-4399-864a-3a3655e3eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/train/images'\n",
    "train_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/train/masks'\n",
    "val_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/val/images'\n",
    "val_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/val/masks'\n",
    "test_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/test/images'\n",
    "test_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/ITER_2/test/masks'\n",
    "\n",
    "# train_image_dir=r'D:\\pvv\\pvv\\data\\jack\\train\\images'\n",
    "# train_label_dir=r'D:\\pvv\\pvv\\data\\jack\\train\\masks'\n",
    "# val_image_dir=r'D:\\pvv\\pvv\\data\\jack\\val\\images'\n",
    "# val_label_dir=r'D:\\pvv\\pvv\\data\\jack\\val\\masks'\n",
    "#test_image_dir=r'D:\\pvv\\pvv\\data\\test\\images'\n",
    "#test_label_dir=r'D:\\pvv\\pvv\\data\\test\\masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847fee8-da64-4abb-b5f3-2c868e8be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir=r'D:\\pvv\\pvv\\data\\train\\images'\n",
    "train_label_dir=r'D:\\pvv\\pvv\\data\\train\\masks'\n",
    "val_image_dir=r'D:\\pvv\\pvv\\data\\val\\images'\n",
    "val_label_dir=r'D:\\pvv\\pvv\\data\\val\\masks'\n",
    "test_image_dir=r'D:\\pvv\\pvv\\data\\test\\images'\n",
    "test_label_dir=r'D:\\pvv\\pvv\\data\\test\\masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57ae38-e8d3-4057-ac1a-a2974373c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/train/images'\n",
    "train_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/train/masks'\n",
    "val_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/val/images'\n",
    "val_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/val/masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2fb93-e920-4a78-b105-bc0cd21755a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    #print(images)\n",
    "    #print(images)\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    #print(images[0].dtype)\n",
    "    \n",
    "    batch = processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "        input_data_format=\"channels_first\"\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    " \n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6436c1-baf0-49fa-9928-9c394645d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom dataset\n",
    "#image_dir = \"path_to_images\"\n",
    "#mask_dir = \"path_to_masks\"\n",
    "train_dataset = CustomDataset(image_dir=train_image_dir, mask_dir=train_label_dir,mapping=class_mapping,processor=processor, transform=transform)\n",
    "val_dataset = CustomDataset(image_dir=val_image_dir, mask_dir=val_label_dir, mapping=class_mapping,processor=processor, transform=transform)\n",
    "\n",
    "train_subset_size = 300\n",
    "val_subset_size=60\n",
    "train_subset, _ = random_split(train_dataset, [train_subset_size, len(train_dataset) - train_subset_size])\n",
    "val_subset, _ = random_split(val_dataset, [val_subset_size, len(val_dataset) - val_subset_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b609383-111f-4a34-8754-95da6312ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(image_dir=test_image_dir, mask_dir=test_label_dir, mapping=class_mapping,processor=processor, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14305c4e-d0ad-4a3a-83db-4f1a785c170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75fc3f-2b6b-42eb-ab8a-f96512b35688",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch[\"mask_labels\"]))\n",
    "print(len(batch[\"class_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c26fd-729e-43bd-a8b9-1c1da3d1d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[\"mask_labels\"][0])\n",
    "print(batch[\"class_labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7707-8843-45d8-b534-5ce090b1e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = batch[\"pixel_values\"][0].numpy()\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65194c42-741e-47be-b26b-7b7cb1a76e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d760e-c9d6-4722-a73b-4ac6fd09b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify class labels\n",
    "labels = [id2label[label] for label in batch[\"class_labels\"][0].tolist()]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de41fdf-0ddd-4bd6-9ce3-bfd4ae717329",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(batch[\"mask_labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45294e6-dfd3-48ee-8d33-ce7c2800eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"mask_labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07ffb0-38a2-4db5-a512-12dcfd8a96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(labels, label_name):\n",
    "  print(\"Label:\", label_name)\n",
    "  idx = labels.index(label_name)\n",
    "\n",
    "  visual_mask = (batch[\"mask_labels\"][0][idx].bool().numpy() * 255).astype(np.uint8)\n",
    "  \n",
    "  return Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b767b-b8be-4bb4-b687-d009976a900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mask(labels, \"guava\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b2380-204c-493f-9b71-7d543b747c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch[\"pixel_values\"],\n",
    "                class_labels=batch[\"class_labels\"],\n",
    "                mask_labels=batch[\"mask_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fcf2d-70e9-48c2-9676-6aec3042ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9b743-2639-4dfc-a12a-3169a60a86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "#metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00272131-227c-4c2d-9009-19ce6395a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "#metric = load_metric(\"mean_iou\")\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "#clf_metrics=evaluate.combine([\"accuracy\",\"f1\",\"precision\",\"recall\"])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr= 0.000392,weight_decay=4.3024e-05)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= 3.97e-05,weight_decay=1.44e-06)\n",
    "\n",
    "#running_loss = 0.0\n",
    "running_train_loss = 0.0\n",
    "running_val_loss = 0.0\n",
    "num_tr_samples = 0\n",
    "num_val_samples = 0\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "running_train_losses = []\n",
    "running_val_losses = []\n",
    "mean_ious = []\n",
    "mean_accs=[]\n",
    "accs=[]\n",
    "num_epochs = 13\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        # Reset the parameter gradients\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "         # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        # outputs = model(\n",
    "        #   pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        #   mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        #   class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        # )\n",
    "    \n",
    "        # Backward propagation\n",
    "        #loss = outputs.loss\n",
    "        loss.backward()\n",
    "    \n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_train_loss += loss.item()\n",
    "        epoch_train_loss+= loss.item()\n",
    "        num_tr_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Training Loss:\", running_train_loss/num_tr_samples)\n",
    "            running_train_losses.append(running_train_loss/num_tr_samples)\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(tqdm(val_loader)):\n",
    "        # if idx > 5:\n",
    "        #   break\n",
    "    \n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "          #outputs = model(pixel_values=pixel_values.to(device))\n",
    "        #   outputs = model(\n",
    "        #   pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        #   mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        #   class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        # )\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            predicted = upsampled_logits.argmax(dim=1)\n",
    "            metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "            #clf_metrics.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "        #loss = outputs.loss\n",
    "        #print(loss)\n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_val_loss += loss.item()\n",
    "        epoch_val_loss+= loss.item()\n",
    "        num_val_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"val Loss:\", running_val_loss/num_val_samples)\n",
    "            running_val_losses.append(running_val_loss/num_val_samples)\n",
    "    \n",
    "        \n",
    "        # get original images\n",
    "        #original_images = batch[\"original_images\"]\n",
    "        #target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        #print(\"target size:\",target_sizes)\n",
    "        # predict segmentation maps\n",
    "        #predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)\n",
    "        # get ground truth segmentation maps\n",
    "        #ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "        #print(\"original mask shape\",ground_truth_segmentation_maps[0].numpy().shape)\n",
    "        #print(\"predicted mask shape\",predicted_segmentation_maps[0].numpy().shape)\n",
    "        #metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "    \n",
    "    epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    final_score=metric.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "    #final_cls_score=clf_metrics.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "    mean_ious.append(final_score['mean_iou'])\n",
    "    mean_accs.append(final_score['mean_accuracy'])\n",
    "    #accs.append(final_cls_score['accuaracy'])\n",
    "    print(f\"Epoch {epoch+1} Mean IoU: {final_score['mean_iou']}\")\n",
    "    print(f\"Epoch {epoch+1} Mean Accuracy: {final_score['mean_accuracy']}\")\n",
    "    #print(f\"Epoch {epoch+1} Accuracy: {final_cls_score['accuracy']}\")\n",
    "\n",
    "    # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "    # so if you're interested, feel free to print them as well\n",
    "    #print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d9bb7-1955-4a50-afaa-7694a31de8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(f\"Epoch {epoch+1} classwise iou: {final_score['per_category_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fdb09-a37b-4c4f-b2e3-7ac3652062de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss and IoU curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss curve\n",
    "plt.plot(range(1, num_epochs + 1), epoch_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), epoch_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../../DATA/OUTPUT/SEGFORMER/RGBH/mit-b4/segformermitb4_epoch_train_val_losses_species_28-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting running Loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss curve\n",
    "plt.plot(range(1, len(running_train_losses) + 1), running_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, len(running_val_losses) + 1), running_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../../DATA/OUTPUT/SEGFORMER/RGBH/mit-b4/segformermitb4_running_train_val_losses_species_28-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Mean IoU curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, num_epochs + 1), mean_ious, label='Mean IoU', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.title('Mean IoU Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../../../DATA/OUTPUT/SEGFORMER/RGBH/mit-b4/IOUmitb4_plot_species_28-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597db44c-0570-4635-bfc6-9ff1184a718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model after fine-tuning\n",
    "model.save_pretrained(\"../../../DATA/MODEL/FINAL/SEGFORMER/RGBH/segformer_mit-b4_uav_tree_species_28-12-24.pt\")\n",
    "#processor.save_pretrained(\"path_to_save_processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c59045-1ee7-4c7b-960c-e334933f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "for epoch in range(100):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_loader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(val_loader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "  # so if you're interested, feel free to print them as well\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb9a6e-cfea-45bd-b251-4260d556a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('D:/pvv/pvv/data/val/images/iihr_2nd_pass_sub_areca_coco2_12_aug_5.TIF')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f936c4b-45d5-419c-ae1c-71966e9363a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor = SegformerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "\n",
    "# prepare the image for the model\n",
    "#pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "#print(pixel_values.shape)\n",
    "#pixel_values = batch[\"pixel_values\"][0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82ca15-d636-4040-9ea5-4375b835a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf50f2f-a9cc-4567-a250-606c55f74f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = batch[\"pixel_values\"][3][:3, :, :].numpy()\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aafa3-8b99-4983-94db-804fc1699438",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60819-7acc-4d20-baf0-1bb698f12fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(batch[\"pixel_values\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc8b26-7a4d-47df-8b63-35db94958051",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = batch[\"original_images\"]\n",
    "target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "# predict segmentation maps\n",
    "#processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=True, do_rescale=True, do_normalize=True)\n",
    "#processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6060a-524c-416e-8dc3-d4ea3693f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[\"original_images\"][3]\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d2f94e4-947f-42eb-9e5a-3f06c7abf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_palette():\n",
    "    \"\"\"Color palette that maps each class to RGB values.\n",
    "    \n",
    "    This one is actually taken from ADE20k.\n",
    "    \"\"\"\n",
    "    return [[180, 180, 180], [191,255,0], [144,238,144], [255,0,255],\n",
    "            [255,182,193], [128,128,0], [255,223,0], [210,180,140],\n",
    "            [255,127,80], [0,255,255], [255,69,0]]\n",
    "\n",
    "palette = color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "93f53f85-b7f7-4d80-aaf6-15cce6dc4c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 7 7 ... 7 7 7]\n",
      " [7 7 7 ... 7 7 7]\n",
      " [7 7 7 ... 7 7 7]\n",
      " ...\n",
      " [7 7 7 ... 7 7 7]\n",
      " [7 7 7 ... 7 7 7]\n",
      " [7 7 7 ... 7 7 7]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n",
    "print(segmentation_map)\n",
    "\n",
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_segmentation_map[segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "ground_truth_color_seg = color_segmentation_map[..., ::-1]\n",
    "#image = np.transpose(image, [2, 1, 0])\n",
    "#img = image * 0.5 + ground_truth_color_seg * 0.5\n",
    "#img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(color_segmentation_map)\n",
    "plt.savefig(\"../../../DATA/OUTPUT/SEGFORMER/RGBH/mit-b5/MODEL_OUTPUT_RGBH/pred_sapota_segformer_mit-b5-IIHR_TRAINING_SAMPLES_SAPOTA_144_aug_0.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c5d57-bb91-471e-9427-31b6349aefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segmentation_map = batch[\"original_segmentation_maps\"][2]\n",
    "\n",
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_segmentation_map[segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "ground_truth_color_seg = color_segmentation_map[..., ::-1]\n",
    "\n",
    "#img = image * 0.5 + ground_truth_color_seg * 0.5\n",
    "#img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(segmentation_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73b511-d48d-4400-b60b-00761f64a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b98938b0-2119-4212-8f83-f9add760416e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded the modified 4-channel MaskFormer model.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def modify_patch_embedding(model, new_in_channels=4):\n",
    "   # Load pre-trained SegFormer\n",
    "   \n",
    "    # Access the patch embedding layer\n",
    "    original_in_channels = 3\n",
    "    new_in_channels = 4  # Example: RGB + Height + Roughness + Gradient\n",
    "    original_patch_embedding = model.segformer.encoder.patch_embeddings[0]\n",
    "    \n",
    "    # Access the internal convolution layer\n",
    "    original_proj = original_patch_embedding.proj\n",
    "    \n",
    "    # Get the number of output channels from the model configuration\n",
    "    out_channels = model.config.hidden_sizes[0]\n",
    "    \n",
    "    # Create a new convolution layer with additional input channels\n",
    "    new_conv = nn.Conv2d(\n",
    "        new_in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=original_proj.kernel_size,\n",
    "        stride=original_proj.stride,\n",
    "        padding=original_proj.padding,\n",
    "        bias=original_proj.bias is not None,\n",
    "    )\n",
    "    \n",
    "    # Copy the weights from the original convolution layer for the first 3 channels\n",
    "    new_conv.weight.data[:, :original_in_channels, :, :] = original_proj.weight.data\n",
    "    # Initialize the weights for the new channels (e.g., random or mean of existing weights)\n",
    "    new_conv.weight.data[:, original_in_channels:, :, :] = original_proj.weight.data.mean(dim=1, keepdim=True)\n",
    "    \n",
    "    # Replace the original convolution layer with the new one\n",
    "    original_patch_embedding.proj = new_conv\n",
    "\n",
    "# Load the configuration\n",
    "model_dir=\"../../../DATA/MODEL/FINAL/SEGFORMER/RGBH/segformer_mit-b5_uav_tree_species_19-12-24.pt\"\n",
    "#model_dir = \"maskformer_swin_base_ade_uav_tree_species.pt\"\n",
    "config = SegformerConfig.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "# Instantiate the model (with a default architecture)\n",
    "model = SegformerForSemanticSegmentation(config)\n",
    "\n",
    "# Modify the architecture for 4-channel input\n",
    "modify_patch_embedding(model)\n",
    "\n",
    "# Load the safetensors state dictionary\n",
    "from safetensors import safe_open\n",
    "\n",
    "safetensors_path = f\"{model_dir}/model.safetensors\"\n",
    "with safe_open(safetensors_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    state_dict = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# Load the modified state dictionary into the model\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model=model.to(device)\n",
    "print(\"Successfully loaded the modified 4-channel MaskFormer model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef7a6f63-d1e2-4e35-b9d4-35bb81d95e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SegformerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b259792-0e87-4fac-a97a-81e6093dfe9a",
   "metadata": {},
   "source": [
    "## Estimate model paprameters and memory for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea52a24-bf8e-4628-a4d7-398c2cc7bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import SegformerForSemanticSegmentation\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "memory_weights = total_params * 4 / (1024 ** 2)  # In MB for float32\n",
    "print(f\"Memory for Weights: {memory_weights:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "722fcf07-f3fc-4599-9c48-2438f8c25e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, we'll use augmentations\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "#model_path = \"../DATA/MODEL/species_model_unet_16-10-24.pt\"\n",
    "def predict(model, image_path, device):\n",
    "    \n",
    "    # transform = transforms.Compose([\n",
    "    # transforms.Resize((512, 512)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    # For training, we'll use augmentations\n",
    "    transform = InferCompose([\n",
    "        InferNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "        #transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    #image = Image.open(test_image_path).convert(\"RGB\")\n",
    "    \n",
    "     # Use Rasterio to open the Geo-referenced image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        \n",
    "        bands=[1,2,3,4]\n",
    "        #bands=[1,2,3]\n",
    "        #bands = src.read((4,3,2))\n",
    "        bands = src.read(bands)\n",
    "        bands = bands.astype(np.float32) # Convert to float for the normalization\n",
    "\n",
    "        # Normalize each band to [0, 1]\n",
    "        bands_normalized = normalize_to_range(bands)\n",
    "\n",
    "        # Rearrange bands into an RGB image\n",
    "        img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "        img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)    \n",
    "        img_tensor = transform(img_tensor).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
    "    \n",
    "    # Pass the preprocessed image through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seg_output= model(img_tensor)\n",
    "       \n",
    "    \n",
    "       \n",
    "    return seg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9150740-5b03-4b0c-8303-922257789f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"../../../DATA/TRAINING_SAMPLES/ITER_2/val/images/IIHR_TRAINING_SAMPLES_SAPOTA_144_aug_0.TIF\"\n",
    "test_mask_path = \"../../../DATA/TRAINING_SAMPLES/ITER_2/val/masks/IIHR_TRAINING_SAMPLES_SAPOTA_144_aug_0.TIF\"\n",
    "predicted_mask = predict(model, test_image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f97249e-eee0-43f7-8b5c-7a90ffbc68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segmentation_maps = processor.post_process_semantic_segmentation(predicted_mask, target_sizes=[(512,512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539510e1-b615-439a-ab99-aa681c4a74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "f1_metric=evaluate.load(\"f1\")\n",
    "precision_metric=evaluate.load(\"precision\")\n",
    "recall_metric=evaluate.load(\"recall\")\n",
    "\n",
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_loader)):\n",
    "    # if idx > 5:\n",
    "    #   break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "       \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "        metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "       \n",
    "        ground_truth_segmentation_maps = torch.cat(\n",
    "            [gt_map.flatten() for gt_map in labels], dim=0\n",
    "        ).cpu().numpy().astype('int32')\n",
    "        \n",
    "        predicted_segmentation_maps = torch.cat(\n",
    "            [torch.tensor(pred_map).flatten() for pred_map in predicted], dim=0\n",
    "        ).cpu().numpy().astype('int32')\n",
    "        #print(type(ground_truth_segmentation_maps))\n",
    "        #print(predicted_segmentation_maps)\n",
    "        f1_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        precision_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        recall_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "\n",
    "final_score=metric.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "final_f1_score=f1_metric.compute(average=None)\n",
    "#mean_f1_score=f1_metric.compute(average=\"macro\")\n",
    "final_precision_score=precision_metric.compute(average=None)\n",
    "#mean_precision_score=precision_metric.compute(average=\"macro\")\n",
    "final_recall_score=recall_metric.compute(average=None)\n",
    "#mean_recall_score=precision_metric.compute(average=\"macro\")\n",
    "#final_cls_score=clf_metrics.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "#mean_ious.append(final_score['mean_iou'])\n",
    "#mean_accs.append(final_score['mean_accuracy'])\n",
    "#accs.append(final_cls_score['accuaracy'])\n",
    "print(f\"Mean IoU: {final_score['mean_iou']}\")\n",
    "print(f\"Mean Accuracy: {final_score['mean_accuracy']}\")\n",
    "print(f\"Classwise iou: {final_score['per_category_iou']}\")\n",
    "print(f\"Classwise accuracy: {final_score['per_category_accuracy']}\")\n",
    "print(f\"Classwise F1: {final_f1_score['f1']}\")\n",
    "print(f\"Mean F1 score: {final_f1_score['f1'].mean()}\")\n",
    "print(f\"Classwise Precision: {final_precision_score['precision']}\")\n",
    "print(f\"Mean Precision score: {final_precision_score['precision'].mean()}\")\n",
    "print(f\"Classwise Recall: {final_recall_score['recall']}\")\n",
    "print(f\"Mean Recall score: {final_recall_score['recall'].mean()}\")\n",
    "#final_score['per_category_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651c82e1-4e68-4902-a44d-b14274d3ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "total_time = 0\n",
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_loader)):\n",
    "    # if idx > 5:\n",
    "    #   break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    #labels = batch[\"mask_labels\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c7494a-3d80-4883-81e1-4c407af239f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Latency per Batch: {total_time / len(test_loader):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302c848-f32e-47c4-924e-7baa96435988",
   "metadata": {},
   "source": [
    "## Per-Sample Latency: Compute the average time per sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccb652-4c24-4589-93b5-fe639157ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(test_dataset)\n",
    "avg_latency = total_time / num_samples\n",
    "print(f\"Average Latency per Sample: {avg_latency * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f47bcb-97a3-4aa9-9a44-6de9fd0177fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
