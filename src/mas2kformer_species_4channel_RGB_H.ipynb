{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e585cc-0c43-4717-bf98-fcb6bb1aad46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9844\\2589122962.py\", line 4, in <module>\n",
      "    from transformers import Mask2FormerForUniversalSegmentation,Mask2FormerImageProcessor,Mask2FormerConfig\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1806, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1805, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1817, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py\", line 36, in <module>\n",
      "    from ...modeling_utils import PreTrainedModel\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py\", line 46, in <module>\n",
      "    from .generation import CompileConfig, GenerationConfig, GenerationMixin\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1805, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py\", line 1817, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py\", line 53, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\candidate_generator.py\", line 26, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py\", line 87, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py\", line 16, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py\", line 295, in <module>\n",
      "    from ._csr import *\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py\", line 11, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.3 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.mask2former.modeling_mask2former because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\utils.py:53\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeam_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcandidate_generator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     54\u001b[0m     AssistedCandidateGenerator,\n\u001b[0;32m     55\u001b[0m     AssistedCandidateGeneratorDifferentTokenizers,\n\u001b[0;32m     56\u001b[0m     CandidateGenerator,\n\u001b[0;32m     57\u001b[0m     EarlyExitCandidateGenerator,\n\u001b[0;32m     58\u001b[0m     PromptLookupCandidateGenerator,\n\u001b[0;32m     59\u001b[0m     _crop_past_key_values,\n\u001b[0;32m     60\u001b[0m     _prepare_attention_mask,\n\u001b[0;32m     61\u001b[0m     _prepare_token_type_ids,\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     64\u001b[0m     NEED_SETUP_CACHE_CLASSES_MAPPING,\n\u001b[0;32m     65\u001b[0m     QUANT_BACKEND_CLASSES_MAPPING,\n\u001b[0;32m     66\u001b[0m     GenerationConfig,\n\u001b[0;32m     67\u001b[0m     GenerationMode,\n\u001b[0;32m     68\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\generation\\candidate_generator.py:26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sklearn_available():\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DynamicCache\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:87\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m     __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     85\u001b[0m     _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     86\u001b[0m )\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:16\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\__init__.py:295\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_csr.py:11\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _spbase, sparray\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     12\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\mask2former\\modeling_mask2former.py:36\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModelOutput, BaseModelOutputWithCrossAttentions\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_greater_or_equal_than_2_1\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_utils.py:46\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileConfig, GenerationConfig, GenerationMixin\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mask2FormerForUniversalSegmentation,Mask2FormerImageProcessor,Mask2FormerConfig\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m load_metric\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1806\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[0;32m   1808\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.mask2former.modeling_mask2former because of the following error (look up to see its traceback):\nFailed to import transformers.generation.utils because of the following error (look up to see its traceback):\nnumpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Mask2FormerForUniversalSegmentation,Mask2FormerImageProcessor,Mask2FormerConfig\n",
    "from evaluate import load as load_metric\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb  # for logging\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import os\n",
    "import rasterio\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import evaluate\n",
    "# Initialize WandB in offline mode\n",
    "#wandb.init(mode=\"offline\")\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "from torch.utils.data import random_split\n",
    "from shapely.geometry import shape\n",
    "from rasterio.features import shapes\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "from scipy.ndimage import label\n",
    "from shapely.geometry import box\n",
    "from skimage.transform import resize\n",
    "from rasterio.mask import mask\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from patchify import patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eff04b6-4f2b-45a1-bafe-c113ecef665f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc84444-77ee-4702-8581-8aad88bcdb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        for t in self.transforms:\n",
    "            image, mask = t(image, mask)\n",
    "        return image, mask\n",
    "\n",
    "class DualResize:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "     \n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        return transforms.functional.resize(img, self.size), transforms.functional.resize(mask, self.size)\n",
    "\n",
    "class DualToPILImage:\n",
    "    def __call__(self, img, mask):\n",
    "        return transforms.ToPILImage()(img), transforms.ToPILImage()(mask)\n",
    "\n",
    "class DualRandomHorizontalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.functional.hflip(img), transforms.functional.hflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class DualRandomVerticalFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        if torch.rand(1) < self.p:\n",
    "            return transforms.functional.vflip(img), transforms.functional.vflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class DualRandomRotation:\n",
    "    def __init__(self, degrees):\n",
    "   \n",
    "        self.degrees = degrees\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        #angle = transforms.RandomRotation.get_params(self.degrees)\n",
    "        #angle = transforms.RandomRotation(10)\n",
    "        angle=10\n",
    "        #print(angle)\n",
    "        return transforms.functional.rotate(img, angle), transforms.functional.rotate(mask, angle)\n",
    "\n",
    "# class ImageColorJitter:\n",
    "#     def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "#         self.transform = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "\n",
    "#     def __call__(self, img, mask):\n",
    "#         return self.transform(img), mask\n",
    "    \n",
    "# class DualNormalize(object):\n",
    "#     def __init__(self, mean, std):\n",
    "#         self.mean = mean\n",
    "#         self.std = std\n",
    "\n",
    "#     def __call__(self, img, mask):\n",
    "#         img = F.to_tensor(img)\n",
    "#         img = transforms.functional.normalize(img, self.mean, self.std)\n",
    "#         return img, mask\n",
    "\n",
    "class DualNormalize:\n",
    "    def __init__(self, mean, std, height_mean=0, height_std=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean: List of mean values for RGB channels.\n",
    "            std: List of standard deviation values for RGB channels.\n",
    "            height_mean: Mean value for the height channel.\n",
    "            height_std: Standard deviation value for the height channel.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.height_mean = height_mean\n",
    "        self.height_std = height_std\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        # Normalize RGB channels\n",
    "        #print(\"typeeeeeeeeeeeee\",type(img))\n",
    "        #img = F.to_tensor(img)\n",
    "        #img = torch.from_numpy(np.array(img).astype(np.float32))\n",
    "        rgb_channels = img[:3, :, :]\n",
    "        rgb_channels = transforms.functional.normalize(rgb_channels, self.mean, self.std)\n",
    "\n",
    "        # Normalize height channel if it exists\n",
    "        if img.shape[0] == 4:  # Check if the 4th channel exists\n",
    "            height_channel = img[3, :, :]\n",
    "            # Apply dataset-specific standardization\n",
    "            height_channel = (height_channel - self.height_mean) / self.height_std\n",
    "            img = torch.cat([rgb_channels, height_channel.unsqueeze(0)], dim=0)\n",
    "        else:\n",
    "            img = rgb_channels\n",
    "        #print(\"image shape\",img.shape)\n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9bf182-9c8d-4ef0-b27a-0413a4ce73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944b79ca-47b3-4d5a-97cb-8a925665f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferNormalize:\n",
    "    def __init__(self, mean, std, height_mean=0, height_std=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean: List of mean values for RGB channels.\n",
    "            std: List of standard deviation values for RGB channels.\n",
    "            height_mean: Mean value for the height channel.\n",
    "            height_std: Standard deviation value for the height channel.\n",
    "        \"\"\"\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.height_mean = height_mean\n",
    "        self.height_std = height_std\n",
    "\n",
    "    def __call__(self, img):\n",
    "       \n",
    "        rgb_channels = img[:3, :, :]\n",
    "        # if rgb_channels.max() <= 1.0 and rgb_channels.min() <= 1.0:\n",
    "        #     #print(f\"De-normalizing RGB channels for {image_name}\")\n",
    "        #     rgb_channels = np.clip(rgb_channels * 255, 0, 255)\n",
    "        # else:\n",
    "        #     # Convert RGB channels to uint8 for augmentation\n",
    "        #     #rgb_channels = np.clip(rgb_channels * 255, 0, 255).astype(np.uint8)\n",
    "        #     rgb_channels = rgb_channels\n",
    "            \n",
    "        rgb_channels = transforms.functional.normalize(rgb_channels, self.mean, self.std)\n",
    "\n",
    "        # Normalize height channel if it exists\n",
    "        if img.shape[0] == 4:  # Check if the 4th channel exists\n",
    "            height_channel = img[3, :, :]\n",
    "            # Apply dataset-specific standardization\n",
    "            height_channel = (height_channel - self.height_mean) / self.height_std\n",
    "            img = torch.cat([rgb_channels, height_channel.unsqueeze(0)], dim=0)\n",
    "        else:\n",
    "            img = rgb_channels\n",
    "        #print(\"image shape\",img.shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2d74fb-b06f-4eb0-96a3-636be1425c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagenet_mean = [0.485, 0.456, 0.406]\n",
    "# imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]  \n",
    "imagenet_std = [0.229, 0.224, 0.225] \n",
    "height_mean=2.06\n",
    "height_std=3.9\n",
    "\n",
    "\n",
    "#imagenet_mean_4ch = [0.485, 0.456, 0.406]  # Replace with the mean values for your dataset\n",
    "#imagenet_std_4ch = [0.229, 0.224, 0.225]  # Replace with the std values for your dataset\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "train_transform = DualCompose([\n",
    "    DualToPILImage(),\n",
    "    DualResize((512, 512)),  # Example size\n",
    "    DualRandomHorizontalFlip(),\n",
    "    DualRandomVerticalFlip(),\n",
    "    DualRandomRotation(10),  # Example max rotation in degrees\n",
    "    #ImageColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "val_transform = DualCompose([\n",
    "    DualToPILImage(),\n",
    "    DualResize((512, 512)),  # Example size\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# For training, we'll use augmentations\n",
    "transform = DualCompose([\n",
    "    #DualToPILImage(),\n",
    "    #DualResize((512, 512)),  # Example size\n",
    "    DualNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "    #transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "455a0675-8003-44b2-882e-5718cdc103af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_to_range(data):\n",
    "#     min_val=data.min()\n",
    "#     max_val=data.max()\n",
    "#     return (data-min_val)/(max_val-min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c44530-b787-491f-8b04-3c643efdc3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_to_range(data):\n",
    "#     # Assume `data` has shape (C, H, W)\n",
    "#     normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "#     for c in range(data.shape[0]):  # Loop over channels\n",
    "#         min_val = data[c].min()\n",
    "#         max_val = data[c].max()\n",
    "#         if max_val == min_val:\n",
    "#             normalized_data[c] = np.zeros_like(data[c])  # Handle case where all values are the same\n",
    "#         else:\n",
    "#             normalized_data[c] = (data[c] - min_val) / (max_val - min_val)\n",
    "#     return normalized_data *255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2e16eb-569b-4f36-9ee8-68bc18ee8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_to_range(data):\n",
    "    \"\"\"\n",
    "    Normalize RGB channels to [0, 255] and process height channel separately.\n",
    "    \n",
    "    Args:\n",
    "        data: Numpy array with shape (C, H, W) where C = 4 (R, G, B, Height).\n",
    "        \n",
    "    Returns:\n",
    "        normalized_data: Numpy array with the same shape as `data`.\n",
    "    \"\"\"\n",
    "    normalized_data = np.zeros_like(data, dtype=np.float32)\n",
    "    for c in range(data.shape[0]):  # Loop over channels\n",
    "        if c < 3:  # For RGB channels\n",
    "            min_val = data[c].min()\n",
    "            max_val = data[c].max()\n",
    "            if max_val == min_val:\n",
    "                normalized_data[c] = np.zeros_like(data[c])  # Handle case where all values are the same\n",
    "            else:\n",
    "                normalized_data[c] = (data[c] - min_val) / (max_val - min_val) \n",
    "                #normalized_data[c] = data[c]\n",
    "        else:  # For the height channel\n",
    "            height_channel = data[c]\n",
    "            height_channel = np.clip(height_channel, a_min=0, a_max=None)  # Clip negative values to 0\n",
    "            # max_val = height_channel.max()\n",
    "            # if max_val > 0:\n",
    "            #     normalized_data[c] = height_channel / max_val  # Scale to [0, 1]\n",
    "            # else:\n",
    "            normalized_data[c] = height_channel  # All zeros if max_val is 0\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e505698-6ccb-403f-a44d-f31a3267aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = [0,1,2, 3, 4, 5,6,7,8,9,10] \n",
    "class_mapping = {val: idx for idx, val in enumerate(class_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9837e-da22-427b-9f09-d83f8f6a9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_mask(mask, mapping):\n",
    "    #print(mask)\n",
    "    # Create a new array to hold the mapped values\n",
    "    remapped_mask = np.zeros_like(mask)\n",
    "    for original, mapped in mapping.items():\n",
    "        remapped_mask[mask == original] = mapped\n",
    "    return remapped_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98c4c40-062b-40fb-9b05-ca2786178aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Define custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, mapping,processor, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        #self.image_filenames = os.listdir(image_dir)\n",
    "        self.image_filenames = list(glob.glob(rf\"{image_dir}\\*.tif\"))\n",
    "        #self.mask_filenames = os.listdir(mask_dir)\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.mapping = mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print( self.image_filenames[idx])\n",
    "        #image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, os.path.basename(image_path))\n",
    "        #mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n",
    "        # Use Rasterio to open the Geo-referenced image\n",
    "        with rasterio.open(image_path) as src:\n",
    "            bands=[1, 2, 3, 4]\n",
    "            #bands=[1, 2, 3]\n",
    "            #bands_data = src.read((4,3,2))\n",
    "            bands_data = src.read(bands)\n",
    "            #bands_data[3,:,:]=np.clip(bands_data[3,:,:],a_min=0,a_max=None)\n",
    "            bands_data = bands_data.astype(np.float32) # Convert to float for the normalization\n",
    "\n",
    "            # Normalize each band to [0, 1]\n",
    "            bands_normalized = normalize_to_range(bands_data)\n",
    "\n",
    "            # Rearrange bands into an RGB image\n",
    "            #img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "            img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "            img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)\n",
    "        # Use Rasterio for masks too, assuming they are Geo-referenced as well\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1).astype(np.uint8)  # Read the mask as numpy array\n",
    "            # Remap the mask\n",
    "            mask = remap_mask(mask, self.mapping)\n",
    "           \n",
    "        original_image =img_tensor\n",
    "        #original_segmentation_map =  torch.from_numpy(mask.copy())\n",
    "        original_segmentation_map = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        if self.transform is not None:\n",
    "            img, mask = self.transform(img_tensor, mask)\n",
    "             \n",
    "            #img = F.to_tensor(img)\n",
    "            mask = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        else:\n",
    "            # Convert numpy arrays to PyTorch tensors\n",
    "            #img = torch.from_numpy(img_tensor)\n",
    "            img= img_tensor\n",
    "            mask = torch.from_numpy(mask.copy())\n",
    "        #print(\"hellllooooooooooooo\",img.shape)\n",
    "\n",
    "        # Process the image and mask using the MaskFormerImageProcessor\n",
    "        #inputs = self.processor(images=img, segmentation_maps=mask, return_tensors=\"pt\")\n",
    "        return (img,mask,original_image, original_segmentation_map)\n",
    "        #return inputs['pixel_values'].squeeze(0), inputs['segmentation_maps'].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abae205-0431-43fc-b71f-6544aa6e3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "\n",
    "repo_id = f\"segments/sidewalk-semantic\"\n",
    "filename = \"id2label.json\"\n",
    "with open(filename) as class_file:\n",
    "  file_contents = class_file.read()\n",
    "id2label = json.loads(file_contents)\n",
    "#id2label = json.load(open(load_dataset(filename)))\n",
    "id2label = {int(k):v for k,v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007481fc-3827-464c-bfd0-e907a211f62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained Mask2Former model\n",
    "model_name = \"facebook/mask2former-swin-large-cityscapes-semantic\"\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(model_name)\n",
    "\n",
    "model=Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-cityscapes-semantic\",\n",
    "                                                          id2label=id2label,\n",
    "                                                          label2id=label2id,\n",
    "                                                          num_labels=11,\n",
    "                                                          ignore_mismatched_sizes=True,local_files_only=True)\n",
    "\n",
    "# Access the patch embedding layer\n",
    "patch_embed = model.model.pixel_level_module.encoder.embeddings.patch_embeddings\n",
    "original_proj = patch_embed.projection\n",
    "\n",
    "# Original and new input channels\n",
    "original_in_channels = 3\n",
    "new_in_channels = 4  # Example: RGB + Height + Roughness + Gradient\n",
    "\n",
    "# Get the number of output channels from the original projection layer\n",
    "out_channels = original_proj.out_channels\n",
    "\n",
    "# Create a new convolution layer with additional input channels\n",
    "new_conv = nn.Conv2d(\n",
    "    new_in_channels,\n",
    "    out_channels,\n",
    "    kernel_size=original_proj.kernel_size,\n",
    "    stride=original_proj.stride,\n",
    "    padding=original_proj.padding,\n",
    "    bias=original_proj.bias is not None,\n",
    ")\n",
    "\n",
    "# Copy the weights from the original convolution layer for the first 3 channels\n",
    "new_conv.weight.data[:, :original_in_channels, :, :] = original_proj.weight.data\n",
    "# Initialize the weights for the new channels (e.g., random or mean of existing weights)\n",
    "new_conv.weight.data[:, original_in_channels:, :, :] = original_proj.weight.data.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Replace the original convolution layer with the new one\n",
    "patch_embed.projection = new_conv\n",
    "\n",
    "print(\"Modified Mask2Former model's patch embedding layer successfully.\")\n",
    "processor = Mask2FormerImageProcessor(ignore_index=255,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4b141-e7d4-4ad8-bd98-151731f00842",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Mask2FormerImageProcessor(ignore_index=255,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f926e87-4a7c-4a39-b1e9-c3fece442b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741cedf-c8a7-40d8-ab21-dd4feab2c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor(torch.randn(3, 512, 512), torch.randn(512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aba29a-d4f8-4399-864a-3a3655e3eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir='C:/PVV/DATA/ITER_2/train/images'\n",
    "train_label_dir='C:/PVV/DATA/ITER_2/train/masks'\n",
    "val_image_dir='C:/PVV/DATA/ITER_2/val/images'\n",
    "val_label_dir='C:/PVV/DATA/ITER_2/val/masks'\n",
    "test_image_dir='C:/PVV/DATA/ITER_2/test/images'\n",
    "test_label_dir='C:/PVV/DATA/ITER_2/test/masks'\n",
    "\n",
    "# train_image_dir=r'D:\\pvv\\pvv\\data\\jack\\train\\images'\n",
    "# train_label_dir=r'D:\\pvv\\pvv\\data\\jack\\train\\masks'\n",
    "# val_image_dir=r'D:\\pvv\\pvv\\data\\jack\\val\\images'\n",
    "# val_label_dir=r'D:\\pvv\\pvv\\data\\jack\\val\\masks'\n",
    "#test_image_dir=r'D:\\pvv\\pvv\\data\\test\\images'\n",
    "#test_label_dir=r'D:\\pvv\\pvv\\data\\test\\masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847fee8-da64-4abb-b5f3-2c868e8be088",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir=r'D:\\pvv\\pvv\\data\\train\\images'\n",
    "train_label_dir=r'D:\\pvv\\pvv\\data\\train\\masks'\n",
    "val_image_dir=r'D:\\pvv\\pvv\\data\\val\\images'\n",
    "val_label_dir=r'D:\\pvv\\pvv\\data\\val\\masks'\n",
    "test_image_dir=r'D:\\pvv\\pvv\\data\\test\\images'\n",
    "test_label_dir=r'D:\\pvv\\pvv\\data\\test\\masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57ae38-e8d3-4057-ac1a-a2974373c338",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/train/images'\n",
    "train_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/train/masks'\n",
    "val_image_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/val/images'\n",
    "val_label_dir='E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/TRAINING_SAMPLES/8BIT/ITER_1/testing/val/masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2fb93-e920-4a78-b105-bc0cd21755a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs = list(zip(*batch))\n",
    "    images = inputs[0]\n",
    "    #print(images)\n",
    "    #print(images)\n",
    "    segmentation_maps = inputs[1]\n",
    "    # this function pads the inputs to the same size,\n",
    "    # and creates a pixel mask\n",
    "    # actually padding isn't required here since we are cropping\n",
    "    #print(images[0].dtype)\n",
    "    \n",
    "    batch = processor(\n",
    "        images,\n",
    "        segmentation_maps=segmentation_maps,\n",
    "        return_tensors=\"pt\",\n",
    "        input_data_format=\"channels_first\"\n",
    "    )\n",
    "\n",
    "    batch[\"original_images\"] = inputs[2]\n",
    "    batch[\"original_segmentation_maps\"] = inputs[3]\n",
    " \n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6436c1-baf0-49fa-9928-9c394645d489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom dataset\n",
    "#image_dir = \"path_to_images\"\n",
    "#mask_dir = \"path_to_masks\"\n",
    "train_dataset = CustomDataset(image_dir=train_image_dir, mask_dir=train_label_dir,mapping=class_mapping,processor=processor, transform=transform)\n",
    "val_dataset = CustomDataset(image_dir=val_image_dir, mask_dir=val_label_dir, mapping=class_mapping,processor=processor, transform=transform)\n",
    "\n",
    "train_subset_size = 300\n",
    "val_subset_size=60\n",
    "train_subset, _ = random_split(train_dataset, [train_subset_size, len(train_dataset) - train_subset_size])\n",
    "val_subset, _ = random_split(val_dataset, [val_subset_size, len(val_dataset) - val_subset_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,collate_fn=collate_fn)\n",
    "#train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b609383-111f-4a34-8754-95da6312ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(image_dir=test_image_dir, mask_dir=test_label_dir, mapping=class_mapping,processor=processor, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14305c4e-d0ad-4a3a-83db-4f1a785c170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed75fc3f-2b6b-42eb-ab8a-f96512b35688",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(batch[\"mask_labels\"]))\n",
    "print(len(batch[\"class_labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20c26fd-729e-43bd-a8b9-1c1da3d1d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[\"mask_labels\"][0])\n",
    "print(batch[\"class_labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7707-8843-45d8-b534-5ce090b1e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = batch[\"pixel_values\"][0].numpy()\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65194c42-741e-47be-b26b-7b7cb1a76e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d760e-c9d6-4722-a73b-4ac6fd09b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify class labels\n",
    "labels = [id2label[label] for label in batch[\"class_labels\"][0].tolist()]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de41fdf-0ddd-4bd6-9ce3-bfd4ae717329",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(batch[\"mask_labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45294e6-dfd3-48ee-8d33-ce7c2800eec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"mask_labels\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd07ffb0-38a2-4db5-a512-12dcfd8a96a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(labels, label_name):\n",
    "  print(\"Label:\", label_name)\n",
    "  idx = labels.index(label_name)\n",
    "\n",
    "  visual_mask = (batch[\"mask_labels\"][0][idx].bool().numpy() * 255).astype(np.uint8)\n",
    "  \n",
    "  return Image.fromarray(visual_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b767b-b8be-4bb4-b687-d009976a900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_mask(labels, \"guava\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384b2380-204c-493f-9b71-7d543b747c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch[\"pixel_values\"],\n",
    "                class_labels=batch[\"class_labels\"],\n",
    "                mask_labels=batch[\"mask_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9fcf2d-70e9-48c2-9676-6aec3042ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9b743-2639-4dfc-a12a-3169a60a86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "#metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00272131-227c-4c2d-9009-19ce6395a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "#metric = load_metric(\"mean_iou\")\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "#clf_metrics=evaluate.combine([\"accuracy\",\"f1\",\"precision\",\"recall\"])\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr= 0.000392,weight_decay=4.3024e-05)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=4.68e-5,weight_decay=3.44e-05)\n",
    "\n",
    "#running_loss = 0.0\n",
    "running_train_loss = 0.0\n",
    "running_val_loss = 0.0\n",
    "num_tr_samples = 0\n",
    "num_val_samples = 0\n",
    "epoch_train_losses = []\n",
    "epoch_val_losses = []\n",
    "running_train_losses = []\n",
    "running_val_losses = []\n",
    "mean_ious = []\n",
    "mean_accs=[]\n",
    "num_epochs = 13\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_train_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        # Reset the parameter gradients\n",
    "        #pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        #labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "         # forward + backward + optimize\n",
    "        #outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        outputs = model(\n",
    "        pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        \n",
    "        # Forward pass\n",
    "        # outputs = model(\n",
    "        #   pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        #   mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        #   class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        # )\n",
    "    \n",
    "        # Backward propagation\n",
    "        #loss = outputs.loss\n",
    "        loss.backward()\n",
    "    \n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_train_loss += loss.item()\n",
    "        epoch_train_loss+= loss.item()\n",
    "        num_tr_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"Training Loss:\", running_train_loss/num_tr_samples)\n",
    "            running_train_losses.append(running_train_loss/num_tr_samples)\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
    "    epoch_train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    for idx, batch in enumerate(tqdm(val_loader)):\n",
    "        # if idx > 5:\n",
    "        #   break\n",
    "    \n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        #labels = batch[\"mask_labels\"]\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "          #outputs = model(pixel_values=pixel_values.to(device))\n",
    "        #   outputs = model(\n",
    "        #   pixel_values=batch[\"pixel_values\"].to(device),\n",
    "        #   mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "        #   class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        # )\n",
    "            # outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            # loss, logits = outputs.loss, outputs.logits\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "                class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            original_images = batch[\"original_images\"]\n",
    "            target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "            predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "            ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "            metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "            #upsampled_logits = nn.functional.interpolate(logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            #predicted = upsampled_logits.argmax(dim=1)\n",
    "            #metric.add_batch(predictions=predicted.detach().cpu().numpy(), references=batch[\"mask_labels\"].detach().cpu().numpy())\n",
    "            #clf_metrics.add_batch(predictions=predicted.detach().cpu().numpy(), references=labels.detach().cpu().numpy())\n",
    "            \n",
    "            \n",
    "        #loss = outputs.loss\n",
    "        #print(loss)\n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_val_loss += loss.item()\n",
    "        epoch_val_loss+= loss.item()\n",
    "        num_val_samples += batch_size\n",
    "        if idx % 100 == 0:\n",
    "            print(\"val Loss:\", running_val_loss/num_val_samples)\n",
    "            running_val_losses.append(running_val_loss/num_val_samples)\n",
    "    \n",
    "        \n",
    "        # get original images\n",
    "        #original_images = batch[\"original_images\"]\n",
    "        #target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        #print(\"target size:\",target_sizes)\n",
    "        # predict segmentation maps\n",
    "        #predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,target_sizes=target_sizes)\n",
    "        # get ground truth segmentation maps\n",
    "        #ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "        #print(\"original mask shape\",ground_truth_segmentation_maps[0].numpy().shape)\n",
    "        #print(\"predicted mask shape\",predicted_segmentation_maps[0].numpy().shape)\n",
    "        #metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "    \n",
    "    epoch_val_loss = epoch_val_loss / len(val_loader)\n",
    "    epoch_val_losses.append(epoch_val_loss)\n",
    "    final_score=metric.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "    #final_cls_score=clf_metrics.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "    mean_ious.append(final_score['mean_iou'])\n",
    "    mean_accs.append(final_score['mean_accuracy'])\n",
    "    #accs.append(final_cls_score['accuaracy'])\n",
    "    print(f\"Epoch {epoch+1} Mean IoU: {final_score['mean_iou']}\")\n",
    "    print(f\"Epoch {epoch+1} Mean Accuracy: {final_score['mean_accuracy']}\")\n",
    "    #print(f\"Epoch {epoch+1} Accuracy: {final_cls_score['accuracy']}\")\n",
    "\n",
    "    # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "    # so if you're interested, feel free to print them as well\n",
    "    #print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d9bb7-1955-4a50-afaa-7694a31de8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(f\"Epoch {epoch+1} classwise iou: {final_score['per_category_accuracy']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655fdb09-a37b-4c4f-b2e3-7ac3652062de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss and IoU curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss curve\n",
    "plt.plot(range(1, num_epochs + 1), epoch_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, num_epochs + 1), epoch_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"D:/pvv/pvv/data/OUTPUT/Mask2Former/RGBH/swin-large-cityscapes/MASK2FORMER_epoch_train_val_losses_species_09-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plotting running Loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training and validation loss curve\n",
    "plt.plot(range(1, len(running_train_losses) + 1), running_train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(range(1, len(running_val_losses) + 1), running_val_losses, label='Validation Loss', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"D:/pvv/pvv/data/OUTPUT/Mask2Former/RGBH/swin-large-cityscapes/MASK2FORMER_running_train_val_losses_species_09-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n",
    "# Plotting Mean IoU curve\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, num_epochs + 1), mean_ious, label='Mean IoU', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean IoU')\n",
    "plt.title('Mean IoU Curve')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"D:/pvv/pvv/data/OUTPUT/Mask2Former/RGBH/swin-large-cityscapes/IOU_plot_species_09-12-24.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597db44c-0570-4635-bfc6-9ff1184a718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model after fine-tuning\n",
    "model.save_pretrained(\"D:/pvv/pvv/data/MODEL/MASK2FORMER/RGBH/mask2former_swin_large_cityscape_uav_tree_species_9-12-24.pt\")\n",
    "#processor.save_pretrained(\"path_to_save_processor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c59045-1ee7-4c7b-960c-e334933f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "running_loss = 0.0\n",
    "num_samples = 0\n",
    "for epoch in range(100):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  model.train()\n",
    "  for idx, batch in enumerate(tqdm(train_loader)):\n",
    "      # Reset the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "          pixel_values=batch[\"pixel_values\"].to(device),\n",
    "          mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "          class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "      )\n",
    "\n",
    "      # Backward propagation\n",
    "      loss = outputs.loss\n",
    "      loss.backward()\n",
    "\n",
    "      batch_size = batch[\"pixel_values\"].size(0)\n",
    "      running_loss += loss.item()\n",
    "      num_samples += batch_size\n",
    "\n",
    "      if idx % 100 == 0:\n",
    "        print(\"Loss:\", running_loss/num_samples)\n",
    "\n",
    "      # Optimization\n",
    "      optimizer.step()\n",
    "\n",
    "  model.eval()\n",
    "  for idx, batch in enumerate(tqdm(val_loader)):\n",
    "    if idx > 5:\n",
    "      break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "      outputs = model(pixel_values=pixel_values.to(device))\n",
    "\n",
    "    # get original images\n",
    "    original_images = batch[\"original_images\"]\n",
    "    target_sizes = [(image.shape[0], image.shape[1]) for image in original_images]\n",
    "    # predict segmentation maps\n",
    "    predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                                  target_sizes=target_sizes)\n",
    "\n",
    "    # get ground truth segmentation maps\n",
    "    ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "    metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "  \n",
    "  # NOTE this metric outputs a dict that also includes the mIoU per category as keys\n",
    "  # so if you're interested, feel free to print them as well\n",
    "  print(\"Mean IoU:\", metric.compute(num_labels = len(id2label), ignore_index = 0)['mean_iou'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb9a6e-cfea-45bd-b251-4260d556a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('D:/pvv/pvv/data/val/images/iihr_2nd_pass_sub_areca_coco2_12_aug_5.TIF')\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f936c4b-45d5-419c-ae1c-71966e9363a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processor = SegformerImageProcessor(ignore_index=0, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "\n",
    "# prepare the image for the model\n",
    "#pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "#print(pixel_values.shape)\n",
    "#pixel_values = batch[\"pixel_values\"][0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e82ca15-d636-4040-9ea5-4375b835a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "for k,v in batch.items():\n",
    "  if isinstance(v, torch.Tensor):\n",
    "    print(k,v.shape)\n",
    "  else:\n",
    "    print(k,v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf50f2f-a9cc-4567-a250-606c55f74f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = batch[\"pixel_values\"][3][:3, :, :].numpy()\n",
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28aafa3-8b99-4983-94db-804fc1699438",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n",
    "ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n",
    "unnormalized_image = (pixel_values * np.array(ADE_STD)[:, None, None]) + np.array(ADE_MEAN)[:, None, None]\n",
    "unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "Image.fromarray(unnormalized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec60819-7acc-4d20-baf0-1bb698f12fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(batch[\"pixel_values\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc8b26-7a4d-47df-8b63-35db94958051",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_images = batch[\"original_images\"]\n",
    "target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "# predict segmentation maps\n",
    "#processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=True, do_rescale=True, do_normalize=True)\n",
    "#processor = MaskFormerImageProcessor(reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)\n",
    "predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed6060a-524c-416e-8dc3-d4ea3693f48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = batch[\"original_images\"][3]\n",
    "Image.fromarray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2f94e4-947f-42eb-9e5a-3f06c7abf2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_palette():\n",
    "    \"\"\"Color palette that maps each class to RGB values.\n",
    "    \n",
    "    This one is actually taken from ADE20k.\n",
    "    \"\"\"\n",
    "    return [[180, 180, 180], [191,255,0], [144,238,144], [255,0,255],\n",
    "            [255,182,193], [128,128,0], [255,223,0], [210,180,140],\n",
    "            [255,127,80], [0,255,255], [255,69,0]]\n",
    "\n",
    "palette = color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f22bd-4d3c-4546-b3f8-73180ca56f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gray_palette():\n",
    "    \"\"\"Color palette that maps each class to RGB values.\n",
    "    \n",
    "    This one is actually taken from ADE20k.\n",
    "    \"\"\"\n",
    "    return [0, 1, 2, 3,4, 5, 6, 7,8, 9, 10]\n",
    "\n",
    "palette = gray_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f53f85-b7f7-4d80-aaf6-15cce6dc4c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n",
    "print(segmentation_map)\n",
    "\n",
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_segmentation_map[segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "ground_truth_color_seg = color_segmentation_map[..., ::-1]\n",
    "#image = np.transpose(image, [2, 1, 0])\n",
    "#img = image * 0.5 + ground_truth_color_seg * 0.5\n",
    "#img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(color_segmentation_map)\n",
    "plt.savefig(\"../../data/OUTPUT/MASK2FORMER/RGBH/swin-large-ade/MODEL_OUTPUT/pred_others_mask2former_swin-large-ade-IIHR_TR_SAMPLES_CLIP_2ND_PASS_PAPAYA_MANGO_106_aug_1.jpg\", bbox_inches='tight',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c5d57-bb91-471e-9427-31b6349aefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "segmentation_map = batch[\"original_segmentation_maps\"][2]\n",
    "\n",
    "color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "for label, color in enumerate(palette):\n",
    "    color_segmentation_map[segmentation_map == label, :] = color\n",
    "# Convert to BGR\n",
    "ground_truth_color_seg = color_segmentation_map[..., ::-1]\n",
    "\n",
    "#img = image * 0.5 + ground_truth_color_seg * 0.5\n",
    "#img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(segmentation_map)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73b511-d48d-4400-b60b-00761f64a651",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d21f0-2877-4319-a8db-fdfbf630e563",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"../../data/MODEL/MASK2FORMER/RGB/swin-large-ade\"\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(model_path+\"/mask2former_uav_tree_species.pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "# Create a preprocessor\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-ade-semantic\",ignore_index=255, reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3512a3-5c54-4feb-95d4-0b226a924085",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def modify_patch_embedding(model, new_in_channels=4):\n",
    "    # Access the patch embedding layer\n",
    "    #patch_embed = model.model.pixel_level_module.encoder.model.embeddings.patch_embeddings\n",
    "    patch_embed = model.model.pixel_level_module.encoder.embeddings.patch_embeddings\n",
    "    original_proj = patch_embed.projection\n",
    "\n",
    "    # Create a new convolution layer with updated input channels\n",
    "    new_conv = torch.nn.Conv2d(\n",
    "        new_in_channels,\n",
    "        original_proj.out_channels,\n",
    "        kernel_size=original_proj.kernel_size,\n",
    "        stride=original_proj.stride,\n",
    "        padding=original_proj.padding,\n",
    "        bias=original_proj.bias is not None,\n",
    "    )\n",
    "\n",
    "    # Initialize new weights\n",
    "    new_conv.weight.data[:, :original_proj.in_channels, :, :] = original_proj.weight.data\n",
    "    new_conv.weight.data[:, original_proj.in_channels:, :, :] = original_proj.weight.data.mean(dim=1, keepdim=True)\n",
    "\n",
    "    # Replace the original projection layer\n",
    "    patch_embed.projection = new_conv\n",
    "\n",
    "# Load the configuration\n",
    "model_dir=\"../../../DATA/MODEL/FINAL/MASK2FORMER/RGBH/mask2former_swin_large_ade_uav_tree_species_7-12-24.pt\"\n",
    "#model_dir = \"maskformer_swin_base_ade_uav_tree_species.pt\"\n",
    "config = Mask2FormerConfig.from_pretrained(model_dir, local_files_only=True)\n",
    "\n",
    "# Instantiate the model (with a default architecture)\n",
    "model = Mask2FormerForUniversalSegmentation(config)\n",
    "\n",
    "# Modify the architecture for 4-channel input\n",
    "modify_patch_embedding(model)\n",
    "\n",
    "# Load the safetensors state dictionary\n",
    "from safetensors import safe_open\n",
    "\n",
    "safetensors_path = f\"{model_dir}/model.safetensors\"\n",
    "with safe_open(safetensors_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "    state_dict = {key: f.get_tensor(key) for key in f.keys()}\n",
    "\n",
    "# Load the modified state dictionary into the model\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "model=model.to(device)\n",
    "print(\"Successfully loaded the modified 4-channel MaskFormer model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b07a3a-2caa-4f0a-855b-08a4c7fa8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Mask2FormerImageProcessor(ignore_index=255,reduce_labels=False, do_resize=False, do_rescale=False, do_normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5cca0-5c49-49db-a5e0-7874259e7f12",
   "metadata": {},
   "source": [
    "## Estimate model paprameters and memory for weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35de08b3-8b97-4412-98d2-6fa663e4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import SegformerForSemanticSegmentation\n",
    "#model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b0-finetuned-ade-512-512')\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "memory_weights = total_params * 4 / (1024 ** 2)  # In MB for float32\n",
    "print(f\"Memory for Weights: {memory_weights:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fcf07-f3fc-4599-9c48-2438f8c25e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training, we'll use augmentations\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "#model_path = \"../DATA/MODEL/species_model_unet_16-10-24.pt\"\n",
    "def predict(model, image_path, device):\n",
    "    \n",
    "    # transform = transforms.Compose([\n",
    "    # transforms.Resize((512, 512)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    # For training, we'll use augmentations\n",
    "    transform = InferCompose([\n",
    "        InferNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "        #transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    #image = Image.open(test_image_path).convert(\"RGB\")\n",
    "    \n",
    "     # Use Rasterio to open the Geo-referenced image\n",
    "    with rasterio.open(image_path) as src:\n",
    "        \n",
    "        bands=[1,2,3,4]\n",
    "        #bands=[1,2,3]\n",
    "        #bands = src.read((4,3,2))\n",
    "        bands = src.read(bands)\n",
    "        bands = bands.astype(np.float32) # Convert to float for the normalization\n",
    "\n",
    "        # Normalize each band to [0, 1]\n",
    "        bands_normalized = normalize_to_range(bands)\n",
    "\n",
    "        # Rearrange bands into an RGB image\n",
    "        img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "        img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)    \n",
    "        img_tensor = transform(img_tensor).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
    "    \n",
    "    # Pass the preprocessed image through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seg_output= model(img_tensor)\n",
    "       \n",
    "    \n",
    "       \n",
    "    return seg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9150740-5b03-4b0c-8303-922257789f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = \"C:/PVV/DATA/ITER_2/val/images/IIHR_TR_SAMPLES_CLIP_2ND_PASS_PAPAYA_MANGO_106_aug_1.TIF\"\n",
    "test_mask_path = \"C:/PVV/DATA/ITER_2/val/masks/IIHR_TR_SAMPLES_CLIP_2ND_PASS_PAPAYA_MANGO_106_aug_1.TIF\"\n",
    "\n",
    "predicted_mask = predict(model, test_image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f97249e-eee0-43f7-8b5c-7a90ffbc68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segmentation_maps = processor.post_process_semantic_segmentation(predicted_mask, target_sizes=[(512,512)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e37d815-da0e-4ac3-87b4-d7012c5de927",
   "metadata": {},
   "source": [
    "## Model Testing with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539510e1-b615-439a-ab99-aa681c4a74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "f1_metric=evaluate.load(\"f1\")\n",
    "precision_metric=evaluate.load(\"precision\")\n",
    "recall_metric=evaluate.load(\"recall\")\n",
    "\n",
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_loader)):\n",
    "    # if idx > 5:\n",
    "    #   break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    #labels = batch[\"mask_labels\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        #loss = outputs.loss\n",
    "        original_images = batch[\"original_images\"]\n",
    "        target_sizes = [(image.shape[1], image.shape[2]) for image in original_images]\n",
    "        predicted_segmentation_maps = processor.post_process_semantic_segmentation(outputs,\n",
    "                                                                              target_sizes=target_sizes)\n",
    "        ground_truth_segmentation_maps = batch[\"original_segmentation_maps\"]\n",
    "\n",
    "        metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        ground_truth_segmentation_maps = torch.cat(\n",
    "            [gt_map.flatten() for gt_map in ground_truth_segmentation_maps], dim=0\n",
    "        ).cpu().numpy().astype('int32')\n",
    "        \n",
    "        predicted_segmentation_maps = torch.cat(\n",
    "            [torch.tensor(pred_map).flatten() for pred_map in predicted_segmentation_maps], dim=0\n",
    "        ).cpu().numpy().astype('int32')\n",
    "        #print(type(ground_truth_segmentation_maps))\n",
    "        #print(predicted_segmentation_maps)\n",
    "        f1_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        precision_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "        recall_metric.add_batch(references=ground_truth_segmentation_maps, predictions=predicted_segmentation_maps)\n",
    "\n",
    "final_score=metric.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "final_f1_score=f1_metric.compute(average=None)\n",
    "#mean_f1_score=f1_metric.compute(average=\"macro\")\n",
    "final_precision_score=precision_metric.compute(average=None)\n",
    "#mean_precision_score=precision_metric.compute(average=\"macro\")\n",
    "final_recall_score=recall_metric.compute(average=None)\n",
    "#mean_recall_score=precision_metric.compute(average=\"macro\")\n",
    "#final_cls_score=clf_metrics.compute(num_labels = len(id2label), ignore_index = 0)\n",
    "#mean_ious.append(final_score['mean_iou'])\n",
    "#mean_accs.append(final_score['mean_accuracy'])\n",
    "#accs.append(final_cls_score['accuaracy'])\n",
    "print(f\"Mean IoU: {final_score['mean_iou']}\")\n",
    "print(f\"Mean Accuracy: {final_score['mean_accuracy']}\")\n",
    "print(f\"Classwise iou: {final_score['per_category_iou']}\")\n",
    "print(f\"Classwise accuracy: {final_score['per_category_accuracy']}\")\n",
    "print(f\"Classwise F1: {final_f1_score['f1']}\")\n",
    "print(f\"Mean F1 score: {final_f1_score['f1'].mean()}\")\n",
    "print(f\"Classwise Precision: {final_precision_score['precision']}\")\n",
    "print(f\"Mean Precision score: {final_precision_score['precision'].mean()}\")\n",
    "print(f\"Classwise Recall: {final_recall_score['recall']}\")\n",
    "print(f\"Mean Recall score: {final_recall_score['recall'].mean()}\")\n",
    "#final_score['per_category_accuracy']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fed73e-cac6-45bf-9f37-cb2d9edda778",
   "metadata": {},
   "source": [
    "## Inference time performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2b783-9d27-4d7d-8ab9-1436dfd32a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "total_time = 0\n",
    "model.eval()\n",
    "for idx, batch in enumerate(tqdm(test_loader)):\n",
    "    # if idx > 5:\n",
    "    #   break\n",
    "\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    #labels = batch[\"mask_labels\"]\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "        )\n",
    "        torch.cuda.synchronize()  # Wait for GPU operations to complete\n",
    "        end_time = time.time()\n",
    "\n",
    "        total_time += (end_time - start_time)\n",
    "\n",
    "print(f\"Total Inference Time: {total_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc109668-cf89-46d4-a384-e787b6f8a23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average Latency per Batch: {total_time / len(test_loader):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51b70f-2029-4915-9a42-b43a84f4ebbb",
   "metadata": {},
   "source": [
    "## Per-Sample Latency: Compute the average time per sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2a90f-d594-4fe4-8a21-6c093f731f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(test_dataset)\n",
    "avg_latency = total_time / num_samples\n",
    "print(f\"Average Latency per Sample: {avg_latency * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035f636-a9d9-40c6-b3b7-f61a12888477",
   "metadata": {},
   "source": [
    "## Dynamic inferencing based on shape file boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f09269-a675-40e4-8e3d-a371cbf8266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"U:/UAV_STUDIES/GKVK Mandya/Mandya-iihr/mandya-rgb-ortho-final.img\"\n",
    "vectorPath=\"E:/PVV/DL-PROJECTS/HORTI/OBJ_2_SPECIES_DL_COMPARISON/DATA/INPUT/STUDYAREA/VECTOR/GENERALIZATION/MANDYA_VC_FARM_AOI_1.shp\"\n",
    "patch_size=512\n",
    "patch_overlap=0\n",
    "image_height=0\n",
    "image_width=0\n",
    "\n",
    "patch_outputs = []\n",
    "predicted_labels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729ec604-a19c-458a-88f4-fb39b4708097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches_rasterio(image_path,height,width,patch_size, overlap):\n",
    "    patches = []\n",
    "#     self.image_height=height\n",
    "#     self.image_width=width\n",
    "#     print(\"image height\", self.image_height)\n",
    "#     print(\"image width\", self.image_width)\n",
    "\n",
    "    # Open the image using rasterio\n",
    "    with rasterio.open(image_path, 'r') as dataset:\n",
    "        width, height = dataset.width, dataset.height\n",
    "        image_height=height\n",
    "        image_width=width\n",
    "        print(\"image height\", self.image_height)\n",
    "        print(\"image width\", self.image_width)\n",
    "\n",
    "        for y in range(0, height, patch_size - overlap):\n",
    "            for x in range(0, width, patch_size - overlap):\n",
    "\n",
    "                # Read the 4 bands (or as many as the image has) \n",
    "                # Note: Indexing in rasterio starts from 1, not 0\n",
    "                window_width = min(patch_size, width - x) \n",
    "                window_height = min(patch_size, height - y)\n",
    "\n",
    "                window = rasterio.windows.Window(x, y, window_width, window_height)\n",
    "                patch = dataset.read(window=window)\n",
    "\n",
    "                patches.append(patch)\n",
    "\n",
    "                # If patch size is less than expected, skip (e.g. for patches at the image boundary)\n",
    "                #if patch.shape[1] != patch_size or patch.shape[2] != patch_size:\n",
    "                    #continue\n",
    "\n",
    "                #patches.append(patch)\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240682a8-e174-478b-b131-92699eae7ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patches_from_section(image_section, patch_size, overlap):\n",
    "    patches = []\n",
    "    # Fetch the dimensions of the image section\n",
    "    num_bands, height, width = image_section.shape\n",
    "    image_height = height\n",
    "    image_width = width\n",
    "    print(\"image section height\", image_height)\n",
    "    print(\"image section width\", image_width)\n",
    "\n",
    "    # Calculate strides based on overlap\n",
    "    stride = patch_size - overlap\n",
    "\n",
    "    for y in range(0, height, stride):\n",
    "        for x in range(0, width, stride):\n",
    "\n",
    "            if y + patch_size > height or x + patch_size > width:\n",
    "                # This is the edge, and the patch would be smaller than patch_size\n",
    "                small_patch = image_section[:, y:min(y + patch_size, height), x:min(x + patch_size, width)]\n",
    "\n",
    "                # Resize the small patch to patch_size x patch_size\n",
    "                #resized_patch = resize(small_patch, (num_bands, patch_size, patch_size), order=0, anti_aliasing=False, preserve_range=True)\n",
    "                #patches.append(resized_patch.astype(image_section.dtype))\n",
    "                patches.append(small_patch)\n",
    "\n",
    "            else:\n",
    "                # Extract patch from the image section\n",
    "                patch = image_section[:, y:y+patch_size, x:x+patch_size]\n",
    "                patches.append(patch)\n",
    "\n",
    "    return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f76efe-9290-4610-8912-b1eab842e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_patches(patches, image_height, image_width, patch_size, overlap):\n",
    "    #stitched_image = np.zeros((image_height, image_width, 3), dtype=np.uint8)\n",
    "    #stitched_image = np.zeros((image_height, image_width,3), dtype=np.uint8)\n",
    "    stitched_image = np.zeros((image_height, image_width), dtype=np.uint8)\n",
    "\n",
    "    patch_idx = 0\n",
    "    print(range(0, image_width, patch_size - overlap))\n",
    "    for y in range(0, image_height, patch_size - overlap):\n",
    "        for x in range(0, image_width, patch_size - overlap):\n",
    "            if(patch_idx<len(patches)):\n",
    "                #print(\"patch index=\",patch_idx)\n",
    "                patch = patches[patch_idx]\n",
    "                #print(\"patch shape=\",np.shape(patch))\n",
    "\n",
    "                 # If the patch is at the border, crop it to fit the original image size\n",
    "                crop_x = patch_size if x + patch_size <= image_width else image_width - x\n",
    "                crop_y = patch_size if y + patch_size <= image_height else image_height - y\n",
    "                patch = patch[:crop_y, :crop_x]\n",
    "\n",
    "                # Combine the patch with the current stitched image\n",
    "                stitched_image[y:y + crop_y, x:x + crop_x] = patch\n",
    "\n",
    "                patch_idx += 1\n",
    "\n",
    "    return stitched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a197c1-fd2d-4fca-8f20-ee3bc5d2127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "#model_path = \"../DATA/MODEL/species_model_unet_16-10-24.pt\"\n",
    "def predict_dynamic(model, patch_bands, device):\n",
    "    \n",
    "    # transform = transforms.Compose([\n",
    "    # transforms.Resize((512, 512)),\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    # For training, we'll use augmentations\n",
    "    transform = InferCompose([\n",
    "        InferNormalize(imagenet_mean, imagenet_std,height_mean,height_std),  # Added normalization\n",
    "        #transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "    #image = Image.open(test_image_path).convert(\"RGB\")\n",
    "    \n",
    "     # Use Rasterio to open the Geo-referenced image\n",
    "    patch_bands = patch_bands.astype(np.float32) # Convert to float for the normalization\n",
    "    bands_normalized = normalize_to_range(patch_bands)\n",
    "    img_normalized = np.transpose(bands_normalized, [1, 2, 0])\n",
    "    #img_normalized = np.transpose(bands_normalized)\n",
    "    img_tensor=torch.from_numpy(img_normalized).permute(2,0,1)  \n",
    "    #img_tensor=torch.from_numpy(img_normalized)   \n",
    "    img_tensor = transform(img_tensor).unsqueeze(0).to(device) # Add batch dimension and move to device\n",
    "   \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        seg_output= model(img_tensor)\n",
    "       \n",
    "    \n",
    "       \n",
    "    return seg_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734d9d0-a018-403d-86b1-9a3b335f3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert bounding box to rasterio window\n",
    "def bbox_to_window(transform, bbox):\n",
    "    left, bottom, right, top = bbox\n",
    "    col_start, row_start = ~transform * (left, top)\n",
    "    col_stop, row_stop = ~transform * (right, bottom)\n",
    "    return ((int(row_start), int(row_stop)), (int(col_start), int(col_stop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d50ead-898b-4e37-83fa-d8dca201c149",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Function to clip the raster using a vector\n",
    "def clip_raster_with_shapefile(raster_path, output_path):\n",
    "    # Load the shapefile using Geopandas\n",
    "    shapefile_path=vectorPath\n",
    "\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    # Convert the GeoDataFrame's geometry to a format rasterio understands\n",
    "    geom = gdf.geometry.values\n",
    "    shapes = [feature.geometry if isinstance(feature, gpd.GeoDataFrame) \n",
    "              else feature for feature in geom]\n",
    "\n",
    "    # Open the raster file\n",
    "    with rasterio.open(raster_path) as src:\n",
    "        # Use the geometry to clip the raster data\n",
    "        out_image, out_transform = mask(src, shapes, crop=True)\n",
    "\n",
    "        # Create the output metadata\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({\n",
    "            \"driver\": \"GTiff\",\n",
    "            \"height\": out_image.shape[1],\n",
    "            \"width\": out_image.shape[2],\n",
    "            \"transform\": out_transform\n",
    "        })\n",
    "\n",
    "        # Write the clipped raster to the output file\n",
    "        with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a2107-631c-4b89-8809-56a0bb714564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertRaster2Polygon(img,vector):\n",
    "    # 1. Load the georeferenced raster image\n",
    "    with rasterio.open(img) as src:\n",
    "        image = src.read(1)  # assuming it's a single-band image\n",
    "\n",
    "        # 2. Extract features from the raster based on unique color codes\n",
    "        mask = image != 0  # adjust this based on your image if needed\n",
    "        features = list(shapes(image, mask=mask, transform=src.transform))\n",
    "\n",
    "        # 3. Convert these features into a GeoDataFrame\n",
    "        geometries = [shape(feat[0]) for feat in features]\n",
    "        values = [feat[1] for feat in features]\n",
    "        gdf = gpd.GeoDataFrame({'geometry': geometries, 'value': values}, crs=src.crs.data)\n",
    "\n",
    "        # 4. Save the GeoDataFrame as a shapefile\n",
    "        gdf.to_file(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b442b5-25e1-4159-a105-cb77c1b606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dominated_by_black(patch, threshold=0.9):\n",
    "    # Consider a pixel \"black\" if all channels are zero\n",
    "    black_pixels = np.sum(np.all(patch == 0, axis=-1))\n",
    "    total_pixels = patch.shape[0] * patch.shape[1]\n",
    "    return (black_pixels / total_pixels) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e3420-b3a7-43dc-a359-c9381f891942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F1\n",
    "large_segmentation_mask=None\n",
    "window=None\n",
    "# Read input raster and prepare an output raster\n",
    "with rasterio.open(img_path, 'r') as src:\n",
    "    image_width, image_height = src.width, src.height\n",
    "    meta = src.meta\n",
    "    #meta['count'] = 3 \n",
    "    meta['count'] = 1  # For single band raster\n",
    "    meta['dtype'] = 'uint8'  # or any other dtype from the list above\n",
    "\n",
    "    #with rasterio.open('IIHR_TEST_OUT.tif', 'w', **meta) as dst:\n",
    "\n",
    "    # Load your vector file with polygons\n",
    "    #gdf = gpd.read_file('data/oct23/DDA_Vacant_land_1600_Plus_31.07.shp')\n",
    "    gdf = gpd.read_file(vectorPath)\n",
    "\n",
    "    gdf_copy=gdf.copy()\n",
    "\n",
    "    # Convert image bounds to a shapely geometry\n",
    "    image_bounds = box(*src.bounds)\n",
    "    # Buffer polygons slightly to allow for merging nearby polygons\n",
    "    gdf_copy['geometry'] = gdf_copy.buffer(0)  # You can adjust this buffer distance based on your data\n",
    "\n",
    "    # Merge overlapping polygons\n",
    "    #merged_polys = list(gdf_copy.unary_union)\n",
    "    union_result = gdf_copy.unary_union\n",
    "\n",
    "    if isinstance(union_result, (Polygon)):\n",
    "        merged_polys = [union_result]\n",
    "    elif isinstance(union_result, (MultiPolygon)):\n",
    "        merged_polys = list(union_result.geoms) \n",
    "    else:\n",
    "        raise ValueError(\"Unexpected geometry type\")\n",
    "    for geom in merged_polys:\n",
    "        #Check if the current polygon intersects with the image\n",
    "        if geom.intersects(image_bounds):\n",
    "  \n",
    "           # Check if the current polygon intersects with the image\n",
    "           # if  gdf_copy['geometry'][0].intersects(image_bounds):\n",
    "            intersection_geom = gdf_copy['geometry'][0].intersection(image_bounds)\n",
    "            #intersection_geom = image_bounds.intersection(gdf_copy['geometry'][0])\n",
    "            #print(\"AOI width\",intersection_geom.bounds)\n",
    "            #print(\"AOI height\",rows_range)\n",
    "    \n",
    "            # Get the window from the bounding box of the intersection\n",
    "            window = bbox_to_window(src.transform, intersection_geom.bounds)\n",
    "    \n",
    "            rows_range, cols_range = window\n",
    "            #print(\"AOI width\",cols_range)\n",
    "            #print(\"AOI height\",rows_range)\n",
    "            if (rows_range[1] - rows_range[0]) < 511:\n",
    "                rows_range = (rows_range[0], rows_range[0] + 511)\n",
    "            if (cols_range[1] - cols_range[0]) < 511:\n",
    "                cols_range = (cols_range[0], cols_range[0] + 511)\n",
    "            window = (rows_range, cols_range)\n",
    "    \n",
    "            # Ensure the window does not exceed the image dimensions\n",
    "            window = (\n",
    "                (max(0, rows_range[0]), min(src.height, rows_range[1])),\n",
    "                (max(0, cols_range[0]), min(src.width, cols_range[1]))\n",
    "            )\n",
    "    \n",
    "            # Use the window parameter in rasterio's read method\n",
    "            image_section = src.read(window=window)\n",
    "            nodata_value = src.nodata  # Get the no-data value\n",
    "             # Replace no-data values with 0\n",
    "            if nodata_value is not None:\n",
    "                image_section = np.where(image_section == nodata_value, 0, image_section)\n",
    "                \n",
    "            rgb_channels = image_section[:3, :, :]  # First three channels\n",
    "            # channel_means = np.mean(rgb_channels, axis=(1, 2))  # Mean per channel\n",
    "            # channel_stds = np.std(rgb_channels, axis=(1, 2))    # Std per channel\n",
    "    \n",
    "            \n",
    "            height_channel = image_section[3, :, :]  # Fourth channel\n",
    "            # Check if the RGB channels are normalized (0-1 range)\n",
    "            if rgb_channels.max() <= 1.0 and rgb_channels.min() <= 1.0:\n",
    "                #print(f\"De-normalizing RGB channels for {image_name}\")\n",
    "                rgb_channels = np.clip(rgb_channels * 255, 0, 255).astype(np.uint8)\n",
    "            else:\n",
    "                # Convert RGB channels to uint8 for augmentation\n",
    "                #rgb_channels = np.clip(rgb_channels * 255, 0, 255).astype(np.uint8)\n",
    "                rgb_channels = rgb_channels.astype(np.uint8)\n",
    "\n",
    "            combined_image = np.vstack([rgb_channels.astype(np.float32), height_channel[np.newaxis, :, :]])  # Shape: (4, H, W)\n",
    "            \n",
    "           \n",
    "            #dst.write(image_section)\n",
    "    \n",
    "            #dim, height, width = image_section.shape\n",
    "            #patches = self.get_patches_rasterio(image_section, height,width,self.patch_size, self.patch_overlap)\n",
    "            patches = get_patches_from_section(combined_image,patch_size, patch_overlap)\n",
    "            #filtered_patches = [(coord, patch) for coord, patch in patches if not is_dominated_by_black(patch)]\n",
    "            patch_outputs=[]\n",
    "            num_bands, height, width = image_section.shape\n",
    "            img_height = height\n",
    "            img_width = width\n",
    "            for patch in patches:\n",
    "                #patch_input = preprocess_input(patch).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    predicted_mask =  predict_dynamic(model, patch, device)\n",
    "                    #predicted_mask = F1.interpolate(predicted_mask, size=(512, 512), mode='bilinear', align_corners=True)\n",
    "                    predicted_segmentation_maps = processor.post_process_semantic_segmentation(predicted_mask, target_sizes=[(512,512)])\n",
    "                    segmentation_map = predicted_segmentation_maps[0].cpu().numpy()\n",
    "                    #print(segmentation_map)\n",
    "                    \n",
    "                    #color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "                    color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1]), dtype=np.uint8) # height, width, 3\n",
    "                    print(\"segmentation map shape\",segmentation_map.shape)\n",
    "                    for label, color in enumerate(palette):\n",
    "                        #color_segmentation_map[segmentation_map == label, :] = color\n",
    "                        color_segmentation_map[segmentation_map == label] = color\n",
    "                    #color_segmentation_map = color_segmentation_map[..., ::-1]\n",
    "                    # plt.figure(figsize=(15, 10))\n",
    "                    # plt.imshow(color_segmentation_map)\n",
    "                    # #plt.savefig(\"../../data/OUTPUT/MASK2FORMER/RGBH/swin-large-ade/MODEL_OUTPUT/pred_others_mask2former_swin-large-ade-IIHR_TR_SAMPLES_CLIP_2ND_PASS_PAPAYA_MANGO_106_aug_1.jpg\", bbox_inches='tight',dpi=500)\n",
    "                    # plt.show()\n",
    "                 \n",
    "                patch_outputs.append(color_segmentation_map)\n",
    "            #print(\"number of output patches\"+str(len(patch_outputs)))\n",
    "            #print(\"img height\"+str(image_height))\n",
    "            #print(\"img width\"+str(image_width))\n",
    "            large_segmentation_mask = stitch_patches(patch_outputs, img_height,width,patch_size, patch_overlap)\n",
    "            large_segmentation_mask = large_segmentation_mask[np.newaxis, ...]\n",
    "            #large_segmentation_mask = large_segmentation_mask[..., ::-1]\n",
    "            image_section_rgb=image_section[:3, :, :]\n",
    "            if image_section_rgb.max() <= 1.0 and image_section_rgb.min() <= 1.0:\n",
    "           \n",
    "                image_section_rgb = np.clip(image_section_rgb * 255, 0, 255).astype(np.uint8)\n",
    "            else:\n",
    "               image_section_rgb = image_section_rgb.astype(np.uint8)\n",
    "        \n",
    "            #image_section_rgb=image_section_rgb.astype(np.uint8)\n",
    "            #print(\"helloooooRGB\",image_section_rgb.shape)\n",
    "            rgb_channels = np.transpose(rgb_channels, [1, 2, 0])\n",
    "          \n",
    "            # img=rgb_channels*0.3 + large_segmentation_mask *0.7\n",
    "            # img = img.astype(np.uint8)\n",
    "            # plt.figure(figsize=(60, 40))\n",
    "            # plt.imshow(img)\n",
    "            # plt.savefig(\"c:/pvv/DLOUTPUT/IIHR_DL_OUTPUT__MASK2FORMER_RGBH_SPECIES_OVERLAY_GRAY_1.jpg\", bbox_inches='tight',dpi=500)\n",
    "            # plt.show()\n",
    "            # plt.figure(figsize=(60, 40))\n",
    "            # plt.imshow(large_segmentation_mask)\n",
    "            # plt.savefig(\"c:/pvv/DLOUTPUT/IIHR_DL_OUTPUT__MASK2FORMER_RGBH_SPECIES_MASK_5.jpg\", bbox_inches='tight',dpi=500)\n",
    "            # plt.show()\n",
    "            #large_segmentation_mask = np.transpose(large_segmentation_mask, [2, 0, 1])\n",
    "            #large_segmentation_mask = large_segmentation_mask.astype(np.uint8)\n",
    "            #large_segmentation_mask = large_segmentation_mask[..., ::-1]\n",
    "            #meta['height'] = img_height\n",
    "            #meta['width']=img_width\n",
    "           \n",
    "    with rasterio.open('c:/pvv/DLOUTPUT/2ND_PASS/MANDYA_DL_INFER_GRAY_1.tif', 'w', **meta) as dst:\n",
    "        dst.write(large_segmentation_mask,window=window)\n",
    "\n",
    "clip_raster_with_shapefile('c:/pvv/DLOUTPUT/2ND_PASS/MANDYA_DL_INFER_GRAY_1.tif', 'c:/pvv/DLOUTPUT/2ND_PASS/MANDYA_DL_INFER_GRAY_1_CLIP.tif')\n",
    "#convertRaster2Polygon('IIHR_TEST_OUT_CLIP.tif','IIHR_DL_OUT_1_clip.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b235c6-a95f-42d1-b8a4-b37bbf14654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convertRaster2Polygon('C:/pvv/DLOUTPUT/2ND_PASS/MOSAIC/iihr_infer_2nd_mosaic.IMG','C:/pvv/DLOUTPUT/2ND_PASS/MOSAIC/iihr_infer_2nd_mosaic.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae074c4-b04b-458a-94f1-c5ea2dc346fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
